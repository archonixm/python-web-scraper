{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping with Beautiful Soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does the Internet Work?\n",
    "\n",
    "The following is a five-minute video on how the Internet works. A basic understanding of this is necessary for us to work with web scraping\n",
    "\n",
    "https://www.youtube.com/watch?v=7_LPdttKXPc\n",
    "\n",
    "To perform HTPP requests in Python we will be working with [requests](http://docs.python-requests.org/en/master/). Fun fact: requests is the most downloaded Python packages of all time, receiving 400,000 downloads in a single day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(\"https://en.wikipedia.org/wiki/List_of_cities_in_Malaysia\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HTTP Response Status\n",
    "\n",
    "To understand if our HTTP requests is successful or not, we have to check the status code. The following [link](\"https://developer.mozilla.org/en-US/docs/Web/HTTP/Status\") explains each of the codes in detail, but generally these are the codes you can expect to see:\n",
    "\n",
    "- 2xx Success (200 means your query was successful)\n",
    "- 3xx Redirections\n",
    "- 4xx Client errors (A familiar code would be error code 404: resource not found )\n",
    "- 5xx Server errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.status_code\n",
    "\n",
    "#You should receive the code '200' here which means that your request was successful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.text) #Calling the text attribute of response allows us to see the HTML text from our query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Scraping 101\n",
    "\n",
    "To Web Scrape is to retrieve data that exists on a website in a usable format for further analysis. Webpages are rendered by your web browser from HTML and CSS code. Useful content for us is usually stores as HTML.\n",
    "\n",
    "In the following section we will perform the following:\n",
    "\n",
    "- Get the HTML code of a given url.  We can use `urllib` or `requests` for that.\n",
    "- Create a Beautiful Soup object which is an interfact to the Document Object Model (DOM)\n",
    "\n",
    "As we know, HTML code is contained in angled brackets '<>'. These brackets provide structural information and are useful for selecting the content we want to see from the entire webpage.\n",
    "\n",
    "For example, '< a >' refers to links on the webpage, and by finding all a tags, one can quickly access all the links on the webpage. BeautifulSoup enables us to select these HTML elements quickly.\n",
    "\n",
    "\n",
    "#### Helpful Resources:\n",
    "- [HTML Basics](\"https://developer.mozilla.org/en-US/docs/Learn/Getting_started_with_the_web/HTML_basics\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beautiful Soup\n",
    "\n",
    "[Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) is a Python library for pulling data out of HTML and XML files. It works with your favorite parser to provide idiomatic ways of navigating, searching, and modifying the parse tree. It commonly saves programmers hours or days of work. The current supported version of Beautiful Soup is version 4.\n",
    "\n",
    "To install:\n",
    "\n",
    "```python\n",
    "!pip install bs4\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bs4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage\n",
    "\n",
    "Right after the installation you can start using BeautifulSoup. At the beginning of your Python script, import the library\n",
    "\n",
    "```python\n",
    "from bs4 import BeautifulSoup\n",
    "```\n",
    "\n",
    "Now you have to pass something to BeautifulSoup to create a soup object. That could be a document or an URL. BeautifulSoup does not fetch the web page for you, you have to do that yourself. Libraries such as `urllib2` or `requests` can be used.\n",
    "\n",
    "```python\n",
    "import requests\n",
    "```\n",
    "\n",
    "**Parser**\n",
    "\n",
    "Beautiful Soup supports the HTML parser included in Python’s standard library, but it also supports a number of third-party Python parsers. One is the lxml parser. Depending on your setup, you might install lxml with one of these commands:\n",
    "\n",
    "```python\n",
    "pip install lxml\n",
    "```\n",
    "or\n",
    "\n",
    "```python\n",
    "pip install html5lib\n",
    "```\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import requests #We already have this from before\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering\n",
    "\n",
    "We can apply filters into methods such as `find_all` and can use these filters based on a tag’s name, on its attributes, on the text of a string, or on some combination of these. This enables us to quickly access HTML elements that we are interested in.\n",
    "\n",
    "**EXAMPLE**\n",
    "\n",
    "Suppose we have the following HTML document:\n",
    "\n",
    "```html\n",
    "html_doc = \"\"\"\n",
    "<html><head><title>The Dormouse's story</title></head>\n",
    "<body>\n",
    "<p class=\"title\"><b>The Dormouse's story</b></p>\n",
    "\n",
    "<p class=\"story\">Once upon a time there were three little sisters; and their names were\n",
    "<a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\">Elsie</a>,\n",
    "<a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\">Lacie</a> and\n",
    "<a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\">Tillie</a>;\n",
    "and they lived at the bottom of a well.</p>\n",
    "\n",
    "<p class=\"story\">...</p>\n",
    "\"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_doc = \"\"\"\n",
    "<html><head><title>The Dormouse's story</title></head>\n",
    "<body>\n",
    "<p class=\"title\"><b>The Dormouse's story</b></p>\n",
    "\n",
    "<p class=\"story\">Once upon a time there were three little sisters; and their names were\n",
    "<a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\">Elsie</a>,\n",
    "<a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\">Lacie</a> and\n",
    "<a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\">Tillie</a>;\n",
    "and they lived at the bottom of a well.</p>\n",
    "\n",
    "<p class=\"story\">...</p>\n",
    "\"\"\"\n",
    "\n",
    "soup = BeautifulSoup(html_doc, 'lxml') #The lxml line here just specifies a HTML parser we want to use\n",
    "print(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A handy function we have to create readable formatting is prettify(). It tidies up the spacing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.prettify()) #It is a lot easier to find elements when things are spaced appropriately"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following bits of code line by line to understand how we can access different tags through BS4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(soup.title) #This will access the 'title' tag\n",
    "#print(soup.title.parent) #This will access the parent tag above the title\n",
    "#print(soup.title.parent.name) #This gives us the name of the parent tag, which is head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try to get the title of the 'webpage' we simulated above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.title)\n",
    "print()\n",
    "print(soup.title.name)\n",
    "print()\n",
    "print(soup.title.text)\n",
    "print()\n",
    "print(soup.title.parent.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the whole content of our soup object with soup.contents. Note that while the output seems similar to just calling 'soup', the two objects are of different data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.contents)\n",
    "print()\n",
    "print()\n",
    "print(type(soup.contents))\n",
    "print() #This just prints empty lines\n",
    "print()\n",
    "print(soup)\n",
    "print()\n",
    "print()\n",
    "print(type(soup))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can specifically access the 'body' tag of our HTML document through soup.body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access different tags such as the paragraph tag '< p >' and the link tags '< a >' in the following way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'class' above just refers to the class attribute which is used to point to a class in a style sheet.\n",
    "\n",
    "If we want to find all of the tags, we can use the 'find_all' method on soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptags = soup.find_all('p')\n",
    "print(len(ptags))\n",
    "ptags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## use find_all for all tags start with 'a' with class 'sister'\n",
    "soup.find_all('a', {'class':'sister'}) #The attribute is passed in as a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all('a', {'class':'sister', 'id':'link1'}) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenge: find the a link with the id: link3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "soup.find_all('a',{'class':'sister','id':'link3'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common task when web scraping is extracting all the URLs found within a webpage's '< a >' tags. We have done so below through the use of a for loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for link in soup.find_all('a'):\n",
    "    print(link.get('href'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply the same logic to extracting the text used as the hyperlink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for para in soup.find_all('a'):\n",
    "    print(para.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another common task is extracting all the text from a page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code only retrieves text from the body tag of our fictitious website\n",
    "\n",
    "print(soup.find('body').get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular Expressions (Regex) \n",
    "\n",
    "\n",
    "We can pass in a regular expression object, Beautiful Soup will filter against\n",
    "that regular expression using its match() method. \n",
    "\n",
    "This code finds all the tags whose names start with the letter \"b\",\n",
    "in this case, the 'body' tag and the 'b' tag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tag in soup.find_all(re.compile(\"^b\")):\n",
    "    print(tag.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tag in soup.find_all(re.compile(\"t\")):\n",
    "    print(tag.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below does not work because we did not use regular expressions. Soup is searching for a tag < t > which does not exist in our fictitious HTML. It would work with \"b\" however."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tag in soup.find_all(\"t\"): #did not use the re package here\n",
    "    print(tag.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List\n",
    "\n",
    "We can pass in a list, Beautiful Soup will allow a string match against any\n",
    "item in that list. \n",
    "\n",
    "This code finds all the 'p' tags and all the 'b' tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.find_all([\"p\", \"b\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Navigating the Parse Tree\n",
    "\n",
    "If you want to know how to navigate the tree please see the official [documentation](http://www.crummy.com/software/BeautifulSoup/bs4/doc/#navigating-the-tree)\n",
    "\n",
    "There you can read about the following things:\n",
    "\n",
    "**Going down**\n",
    "* Navigating using tag names \n",
    " * contents and children\n",
    " * descendants\n",
    " * string\n",
    " * strings and stripped_strings\n",
    "\n",
    "**Going up**\n",
    "* parent\n",
    "* parents\n",
    "\n",
    "**Going sideways**\n",
    "* next_sibling and .previous_sibling \n",
    "* next_siblings and .previous_siblings \n",
    "\n",
    "**Going back and forth**\n",
    "* .next_element and .previous_element\n",
    "* .next_elements and .previous_elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Workflow\n",
    "\n",
    "1. Find the data you want on the web.\n",
    "2. Inspect the webpage and figure out how to select the content you want. This usually involves some combination of\n",
    "    - Viewing the source code of the page (especially if it is simple), and\n",
    "    - Figuring out the structure of the HTML parse tree.  This step is much easier with a something like __Chrome Developer Tools__.\n",
    "3.  Write code to get out what you want:\n",
    "    - If the page is very simple, treat it as a bunch of text => __string manipulation / [regular expressions](https://docs.python.org/2/howto/regex.html)__ in Python.\n",
    "    - To have a more robust solution, it is better to use the HTML parse tree => __[BeautifulSoup](http://www.crummy.com/software/BeautifulSoup/bs4/doc/) / [lxml](http://lxml.de/lxmlhtml.html)__ in Python.\n",
    "4.  Make sure it worked!\n",
    "5.  If your crawling problem is at all non-trivial, you will now have to go back to Step 2 to zoom in further -- or you'll have parsed the URL of a link you want to follow, in which case you'll go back to Step 1 to figure out how to parse what you want from the new target page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "Given the following page from https://en.wikipedia.org/wiki/List_of_cities_in_Malaysia, extract all the filenames and their links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://en.wikipedia.org/wiki/List_of_cities_in_Malaysia\"\n",
    "    \n",
    "response1 = requests.get(url)\n",
    "soup1 = BeautifulSoup(response1.text, 'lxml')\n",
    "print(soup1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 1\n",
    "\n",
    "The first thing you should do from here is prettify your soup1 and inspect the general structure "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup1.prettify())\n",
    "\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 2\n",
    "\n",
    "Retrieve the title in string format of the webpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "print(soup1.title.string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 3\n",
    "\n",
    "Retrieve all the links on your website. Remember that these are contained in the 'a' tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "for link in soup1.find_all('a'):\n",
    "    print(link.get('href'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 4 \n",
    "\n",
    "Find all the tables, and then find the specific table that contains the state names that we are interested in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup1.find_all('table') #You can actually check this by going \n",
    "#to the website and right clicking on the table you are interested in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "right_table = soup1.find('table', class_ = 'wikitable sortable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "right_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list1 = []\n",
    "\n",
    "for link in right_table.find_all('a'):\n",
    "    list1.append(link.get('title'))\n",
    "    \n",
    "list1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(list1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.iloc[[2,7,10,13,17,20,23,26,29,32,35,38,41,44]] \n",
    "\n",
    "#Just pulling out the relevant states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.rename(columns={0:'State'})\n",
    "#df1.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do it yourself: Web Scraping\n",
    "\n",
    "The goal of this mini-project is to scrape data from e-commerce or other websites such as\n",
    "Lelong, Lazada, Mudah, iProperty, Booking, Expedia etc.\n",
    "\n",
    "Scrape at least 1000 items from one of the website mentioned above. The scraped data should include:\n",
    "- Product Name/Product Title\n",
    "- Amount/Price \n",
    "- Brand\n",
    "- Comments/Reviews\n",
    "- Number of views\n",
    "\n",
    "\n",
    "In addition, you are required to export the scraped data to dataframe format and also save a\n",
    "copy in csv format. \n",
    "\n",
    "Upon successful extracting data to dataframe, you are required to do a data\n",
    "analysis on the data.\n",
    "\n",
    "Your analysis should provide answers to the following questions: \n",
    "* What do you think is interesting about this data? \n",
    "* Tell a story about some interesting thing you have discovered by looking at the data. \n",
    "* Visualize your data with matplotlib or with folium library package.\n",
    "\n",
    "For example, you might consider whether there is a difference in pricings at different times\n",
    "doing the day or city, or whether other factors that influnced the pricings etc. Another thing you\n",
    "might consider is whether there is a relationship between the pricing and number of reviews or\n",
    "comments.\n",
    "\n",
    "Get your analysis workflow in your Jupyter notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time package\n",
    "\n",
    "These websites have algorithms to detect people that may be accessing large amounts of their data in a rapid fashion. Time helps us add a human-like pause to our code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to add **sleeps** in order not to be blacklisted by the website we are crawling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "for i in range(10):\n",
    "    print(i)\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will scrape a lelong url and write the information into a CSV for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "lelong_url='https://www.lelong.com.my/catalog/all/list'#this is the url we will look at\n",
    "\n",
    "with open('phones_lelong.csv', 'w', encoding='utf-8', newline='') as csvfile:\n",
    "    lelongwriter = csv.writer(csvfile)\n",
    "    \n",
    "# This is a context manager to open a file and write to it\n",
    "    \n",
    "    for page in range(1, 11):\n",
    "        print(\"Querying page %s...\" % page)\n",
    "        response = requests.get(lelong_url, params={'TheKeyword':'phone', 'D': page})\n",
    "        print('Got page %s' % page)\n",
    "        soup = BeautifulSoup(response.text, 'lxml')\n",
    "        results = soup.find_all('div', attrs={'class':'summary'})\n",
    "        for product in results:\n",
    "            b_element = product.find('b')\n",
    "            price = float(b_element.get('data-price'))\n",
    "            url = b_element.get('data-link')\n",
    "            title = b_element.text\n",
    "            lelongwriter.writerow([title, price, url])\n",
    "        print('Sleeping...')\n",
    "        time.sleep(1)\n",
    "        print('Waking up!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df2 = pd.read_csv('phones_lelong.csv', header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.rename(columns = {0:'Summary', 1:'Price', 2:'Hyperlink'}, inplace = True)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some useful information about price from the data\n",
    "\n",
    "print(df2.Price.mean())\n",
    "print(df2.Price.max())\n",
    "print(df2.Price.min())\n",
    "print(len(df2)) # we have this many rows in our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brandlist = ['[sS]amsung','[sS]ony','[sS]pigen','[hH]uawei','[cC]elcom',\n",
    "             '[aA]pple','[lL]G','[mM]otorola','[rR]azer','[pP]anasonic','[xX]iaomi','3M',\n",
    "             '[oO]nePlus','[oO]ppo']\n",
    "\n",
    "# The reason why we have [sS] is to enable string matching \n",
    "#for either upper or lower case in the name.\n",
    "\n",
    "brandcounts = []\n",
    "for brand in brandlist:\n",
    "    \n",
    "    brandcounts.append(len(df2[df2.Summary.str.contains(brand)]))\n",
    "    \n",
    "dict1 = dict(zip(brandlist,brandcounts))\n",
    "df3 = pd.DataFrame.from_dict(dict1, orient = 'index')\n",
    "df3.columns = ['Count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df3.Count.sum())\n",
    "len(df2)\n",
    "# There may be unbranded goods or goods with brands we are not aware of"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.plot(kind = 'barh', title = 'Brands by Count')\n",
    "\n",
    "# Note that this graph will look different every day because we are donwloading fresh data!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "import datetime as dt\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from sqlalchemy import create_engine\n",
    "from headers import headers_list\n",
    "from data_skills import DATA_SKILLS\n",
    "from skill_extraction import extract_skills, extract_ignore, extract_data_skills\n",
    "from secrets import settings, api_keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JMLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jmlr_scraper(engine):\n",
    "    base_url = 'https://jmlr.org'\n",
    "    url = base_url + '/papers/v22/'\n",
    "    page = requests.get(url, headers=random.choice(headers_list))\n",
    "    if page.status_code != 200:\n",
    "        return\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    dls = soup.findAll('dl')\n",
    "    # Get existing papers in database\n",
    "    df_ex = pd.read_sql_query('select cj.id, cj.title from \"ContentJMLR\" cj', engine)\n",
    "    ex_papers = df_ex['title'].unique().tolist()\n",
    "    papers = []\n",
    "    # Iterate through each paper\n",
    "    has_new = False\n",
    "    for dl in dls:\n",
    "        title = dl.find('dt').get_text()\n",
    "        if title in ex_papers:\n",
    "            continue\n",
    "        paper = {}\n",
    "        dd = dl.find('dd')\n",
    "        paper['title'] = title\n",
    "        paper['authors'] = dd.get_text().split(';')[0].strip()\n",
    "        paper['journal_num'] = dd.get_text().split(';')[-1].split('\\n')[0].strip()\n",
    "        for a in dd.findAll('a'):\n",
    "            if a.get_text() == '(Machine Learning Open Source Software Paper)':\n",
    "                continue\n",
    "            href = a['href']\n",
    "            if 'http' not in href:\n",
    "                href = 'https://jmlr.org' + href\n",
    "            paper[a.get_text()] = href\n",
    "        # Get abstract of paper and extract skills\n",
    "        output = get_abstract_skills(paper)\n",
    "        if output is not None:\n",
    "            paper['abstract'] = output[0]\n",
    "            if len(output[1]) > 0:\n",
    "                paper['skills'] = '; '.join(output[1])\n",
    "                data_skills = extract_data_skills(output[1])\n",
    "                if len(data_skills) > 0:\n",
    "                    paper['data_skills'] = '; '.join(data_skills)\n",
    "        papers.append(paper)\n",
    "        has_new = True\n",
    "    # Compile into dataframe if we have new papers\n",
    "    if has_new:\n",
    "        df = pd.DataFrame.from_dict(papers)\n",
    "        df['id'] = df.index + max(df_ex['id']) + 1\n",
    "        return df\n",
    "    return None\n",
    "\n",
    "def get_abstract_skills(paper):\n",
    "    page = requests.get(paper['abs'], headers=random.choice(headers_list))\n",
    "    if page.status_code != 200:\n",
    "        return None\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    abstract = soup.find('p', class_='abstract').get_text().strip('\\n')\n",
    "    all_skills = extract_skills(paper['title'] + ' ' + abstract)\n",
    "    keep_skills, _ = extract_ignore(all_skills)\n",
    "    keep_skills.sort()\n",
    "    return abstract, keep_skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(settings['skills_db'])\n",
    "df_jmlr = jmlr_scraper(engine)\n",
    "df_jmlr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.dispose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jmlr.to_csv('database/jmlr.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Youtube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_youtube_videos(skill, filter_time=None):\n",
    "    base_url = 'https://www.youtube.com'\n",
    "    # Dictionary for filtering search query\n",
    "    sp_dict = {'this_year': 'EgQIBRAB', 'this_month': 'EgQIBBAB', 'this_week': 'EgQIAxAB', 'today': 'EgQIAhAB'}\n",
    "    if filter_time not in sp_dict.keys():\n",
    "        return None\n",
    "    url = base_url + '/results'\n",
    "    query = 'learn ' + skill\n",
    "    params = {'search_query': query.replace(' ', '+')}\n",
    "    # Default is no filter\n",
    "    if filter_time is not None:\n",
    "        params['sp'] = sp_dict[filter_time]\n",
    "    page = requests.get(url, params=params, headers=random.choice(headers_list))\n",
    "    if page.status_code != 200:\n",
    "        print(page, page.reason)\n",
    "        return None\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    json_text = str(soup.find_all('script')).split('var ytInitialData = ')[-1].split(';</script>')[0]\n",
    "    res = json.loads(json_text)\n",
    "    res = res['contents']['twoColumnSearchResultsRenderer']['primaryContents']['sectionListRenderer']['contents']\n",
    "    video_list = []\n",
    "    # Iterate through each video\n",
    "    for contents in res:\n",
    "        # Get only those with video\n",
    "        if 'itemSectionRenderer' not in contents:\n",
    "            continue\n",
    "        contents = contents['itemSectionRenderer']['contents']\n",
    "        for content in contents:\n",
    "            # Ignore ads\n",
    "            if 'videoRenderer' not in content:\n",
    "                continue\n",
    "            content = content['videoRenderer']\n",
    "            title = get_text(content, 'title')\n",
    "            if title is None:\n",
    "                continue\n",
    "            description = get_description(content)\n",
    "            skills, data_skills = get_skills(title, description)\n",
    "            published_year, published_month = get_published_date(content)\n",
    "            video_list.append({\n",
    "                'id': content['videoId'],\n",
    "                'title': title,\n",
    "                'channel': get_text(content, 'ownerText'),\n",
    "                'published_year': published_year,\n",
    "                'published_month': published_month,\n",
    "                'length': get_length(content),\n",
    "                'view_count': get_view_count(content),\n",
    "                'url': get_url(content),\n",
    "                'description': description,\n",
    "                'skills': skills,\n",
    "                'data_skills': data_skills\n",
    "            })\n",
    "    df = pd.DataFrame.from_dict(video_list)\n",
    "    # df['length'] = pd.to_timedelta(df['length'])\n",
    "    return df\n",
    "\n",
    "def get_text(content, info):\n",
    "    try:\n",
    "        return ' '.join(t['text'] for t in content[info]['runs'])\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def get_length(content):\n",
    "    try:\n",
    "        length = content['lengthText']['simpleText']\n",
    "        length = length.split(':')\n",
    "        if len(length) == 1:\n",
    "            length.insert(0, '00')\n",
    "        if len(length) == 2:\n",
    "            length.insert(0, '00')\n",
    "        return ':'.join(length)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def get_published_date(content):\n",
    "    try:\n",
    "        published_time = content['publishedTimeText']['simpleText']\n",
    "        val = [int(s) for s in published_time.split() if s.isdigit()][0]\n",
    "        current = dt.datetime.now()\n",
    "        if 'year' in published_time:\n",
    "            published = current - dt.timedelta(days=365.25*val)\n",
    "        elif 'month' in published_time:\n",
    "            published = current - dt.timedelta(days=30.436875*val)\n",
    "        elif 'week' in published_time:\n",
    "            published = current - dt.timedelta(weeks=val)\n",
    "        elif 'day' in published_time:\n",
    "            published = current - dt.timedelta(days=val)\n",
    "        elif 'hour' in published_time:\n",
    "            published = current - dt.timedelta(hours=val)\n",
    "        elif 'minute' in published_time:\n",
    "            published = current - dt.timedelta(minutes=val)\n",
    "        elif 'second' in published_time:\n",
    "            published = current - dt.timedelta(seconds=val)\n",
    "        return published.year, published.month\n",
    "    except:\n",
    "        return None, None\n",
    "\n",
    "def get_view_count(content):\n",
    "    try:\n",
    "        view_count = content['viewCountText']['simpleText']\n",
    "        view_count = view_count.split(' views')[0].replace(',', '')\n",
    "        return int(view_count)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def get_url(content):\n",
    "    try:\n",
    "        url = content['navigationEndpoint']['commandMetadata']['webCommandMetadata']['url']\n",
    "        return base_url + url\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def get_description(content):\n",
    "    try:\n",
    "        description = ' '.join([t['text'] for t in content['detailedMetadataSnippets'][0]['snippetText']['runs']])\n",
    "        return description\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def get_skills(title, description):\n",
    "    context = title\n",
    "    if description is not None:\n",
    "        context = context + ' ' + description\n",
    "    all_skills = extract_skills(context)\n",
    "    # Ignore the Video skill as it is not relevant for Youtube\n",
    "    if 'Video' in all_skills:\n",
    "        all_skills.remove('Video')\n",
    "    keep_skills, _ = extract_ignore(all_skills)\n",
    "    keep_skills.sort()\n",
    "    if len(keep_skills) > 0:\n",
    "        data_skills = extract_data_skills(keep_skills)\n",
    "        if len(data_skills) > 0:\n",
    "            return '; '.join(keep_skills), '; '.join(data_skills)\n",
    "        return '; '.join(keep_skills), None\n",
    "    return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yt = pd.DataFrame()\n",
    "\n",
    "for skill in DATA_SKILLS:\n",
    "    try:\n",
    "        df_temp = get_youtube_videos(skill, 'this_month')\n",
    "        df_yt = df_yt.append(df_temp)\n",
    "    except Exception as e:\n",
    "        print('Error in scraping Youtube for {}'.format(skill), e)\n",
    "    time.sleep(5)\n",
    "\n",
    "df_yt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(settings['skills_db2'])\n",
    "df_yt = pd.read_csv('database/youtube.csv')\n",
    "df_yt['length'] = df_yt['length'].apply(lambda x: x.split()[-1])\n",
    "df_yt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yt.to_sql('ContentYoutube', engine, index=False, if_exists='replace')\n",
    "engine.dispose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_yt), len(df_yt[~df_yt['data_skills'].isna()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Medium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def medium_scraper(tag, date):\n",
    "    base_url = 'https://medium.com/tag/{}/archive/'\n",
    "    url = base_url.format(tag) + date.strftime('%Y/%m/%d')\n",
    "    page = requests.get(url, headers=random.choice(headers_list))\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    # Pulls each card from the feed. Each card is a story or comment\n",
    "    cards = soup.find_all('div', class_='streamItem streamItem--postPreview js-streamItem')\n",
    "    card_list = []\n",
    "    for card in cards:\n",
    "        title = get_title(card)\n",
    "        subtitle = get_subtitle(card)\n",
    "        claps = get_claps(card)\n",
    "        if title is None or is_comment(card) or claps is None:\n",
    "            continue\n",
    "        if claps < 100:\n",
    "            continue\n",
    "        skills, data_skills = get_skills(title, subtitle)\n",
    "        card_list.append({\n",
    "            'id': get_id(card),\n",
    "            'title': title,\n",
    "            'subtitle': subtitle,\n",
    "            'author': get_author(card),\n",
    "            'publication': get_publication(card),\n",
    "            'published_date': date,\n",
    "            'read_time_mins': get_read_time(card),\n",
    "            'claps': claps,\n",
    "            'url': get_url(card),\n",
    "            'skills': skills,\n",
    "            'data_skills': data_skills,\n",
    "        })\n",
    "    df = pd.DataFrame.from_dict(card_list)\n",
    "    return df\n",
    "\n",
    "def get_id(card):\n",
    "    id_ = card.find('div', class_='postArticle postArticle--short js-postArticle js-trackPostPresentation js-trackPostScrolls')\n",
    "    if id_ is not None:\n",
    "        return id_['data-post-id']\n",
    "    return id_\n",
    "\n",
    "def get_title(card):\n",
    "    # Different combination of classes possible for titles\n",
    "    combinations = [('h3', 'graf graf--h3 graf-after--figure graf--title'),\n",
    "                    ('h3', 'graf graf--h3 graf-after--figure graf--trailing graf--title'),\n",
    "                    ('h4', 'graf graf--h4 graf--leading'),\n",
    "                    ('h3', 'graf graf--h3 graf--leading graf--title'),\n",
    "                    ('p', 'graf graf--p graf--leading'),\n",
    "                    ('h3', 'graf graf--h3 graf--startsWithDoubleQuote graf--leading graf--title'),\n",
    "                    ('h3', 'graf graf--h3 graf--startsWithDoubleQuote graf-after--figure graf--trailing graf--title')]\n",
    "    title = None\n",
    "    for combi in combinations:\n",
    "        title = card.find(combi[0], class_=combi[1])\n",
    "        if title is not None:\n",
    "            return title.text\n",
    "    return title\n",
    "\n",
    "def get_subtitle(card):\n",
    "    # Different combination of classes possible for subtitles\n",
    "    combinations = [('h4', 'graf graf--h4 graf-after--h3 graf--subtitle'),\n",
    "                    ('h4', 'graf graf--h4 graf-after--h3 graf--trailing graf--subtitle'),\n",
    "                    ('strong', 'markup--strong markup--p-strong'),\n",
    "                    ('h4', 'graf graf--p graf-after--h3 graf--trailing'),\n",
    "                    ('p', 'graf graf--p graf-after--h3 graf--trailing'),\n",
    "                    ('blockquote', 'graf graf--pullquote graf-after--figure graf--trailing'),\n",
    "                    ('p', 'graf graf--p graf-after--figure'),\n",
    "                    ('blockquote', 'graf graf--blockquote graf-after--h3 graf--trailing'),\n",
    "                    ('p', 'graf graf--p graf-after--figure graf--trailing'),\n",
    "                    ('em', 'markup--em markup--p-em'),\n",
    "                    ('p', 'graf graf--p graf-after--p graf--trailing')]\n",
    "    subtitle = None\n",
    "    for combi in combinations:\n",
    "        subtitle = card.find(combi[0], class_=combi[1])\n",
    "        if subtitle is not None:\n",
    "            return subtitle.text\n",
    "    return subtitle\n",
    "\n",
    "def get_author(card):\n",
    "    author = card.find('a', class_='ds-link ds-link--styleSubtle link link--darken link--accent u-accentColor--textNormal u-accentColor--textDarken')\n",
    "    if author is not None:\n",
    "        return author.text\n",
    "    return author\n",
    "\n",
    "def get_publication(card):\n",
    "    pub = card.find('a', class_='ds-link ds-link--styleSubtle link--darken link--accent u-accentColor--textNormal')\n",
    "    if pub is not None:\n",
    "        return pub.text\n",
    "    return pub\n",
    "\n",
    "def get_read_time(card):\n",
    "    time = card.find('span', class_='readingTime')\n",
    "    if time is not None:\n",
    "        time = time['title']\n",
    "        return time.replace(' min read', '')\n",
    "    return time\n",
    "\n",
    "def get_claps(card):\n",
    "    claps = card.find('button', class_='button button--chromeless u-baseColor--buttonNormal js-multirecommendCountButton u-disablePointerEvents')\n",
    "    if claps is not None:\n",
    "        claps = claps.text\n",
    "        if 'K' in claps:\n",
    "            try:\n",
    "                return int(float(claps.replace('K', '')) * 1000)\n",
    "            except:\n",
    "                return None\n",
    "        else:\n",
    "            try:\n",
    "                return int(claps)\n",
    "            except:\n",
    "                return None\n",
    "    return claps\n",
    "\n",
    "def is_comment(card):\n",
    "    # Check if card is a story or comment\n",
    "    comment = card.find('div', class_='u-fontSize14 u-marginTop10 u-marginBottom20 u-padding14 u-xs-padding12 u-borderRadius3 u-borderCardBackground u-borderLighterHover u-boxShadow1px4pxCardBorder')\n",
    "    return comment is not None\n",
    "\n",
    "def get_url(card):\n",
    "    url = card.find('a', class_='')\n",
    "    if url is not None:\n",
    "        return url['href'].split('?')[0]\n",
    "    return url\n",
    "\n",
    "def get_skills(title, subtitle):\n",
    "    context = title\n",
    "    if subtitle is not None:\n",
    "        context = context + ' ' + subtitle\n",
    "    all_skills = extract_skills(context)\n",
    "    keep_skills, _ = extract_ignore(all_skills)\n",
    "    keep_skills.sort()\n",
    "    if len(keep_skills) > 0:\n",
    "        data_skills = extract_data_skills(keep_skills)\n",
    "        if len(data_skills) > 0:\n",
    "            return '; '.join(keep_skills), '; '.join(data_skills)\n",
    "        return '; '.join(keep_skills), None\n",
    "    return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag = 'data-science'\n",
    "start_date = dt.datetime(2021, 9, 30)\n",
    "end_date = dt.datetime(2021, 10, 18)\n",
    "current_date = start_date\n",
    "\n",
    "for i in range((end_date - start_date).days):\n",
    "    df = medium_scraper(tag, current_date)\n",
    "    if i == 0:\n",
    "        df.to_csv('database/medium_data_science.csv', index=False)\n",
    "    else:\n",
    "        df.to_csv('database/medium_data_science.csv', index=False, mode='a', header=False)\n",
    "    current_date = current_date + dt.timedelta(days=1)\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag = 'machine-learning'\n",
    "current_date = start_date\n",
    "\n",
    "for i in range((end_date - start_date).days):\n",
    "    df = medium_scraper(tag, current_date)\n",
    "    if i == 0:\n",
    "        df.to_csv('database/medium_machine_learning.csv', index=False)\n",
    "    else:\n",
    "        df.to_csv('database/medium_machine_learning.csv', index=False, mode='a', header=False)\n",
    "    current_date = current_date + dt.timedelta(days=1)\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag = 'data-engineering'\n",
    "current_date = start_date\n",
    "\n",
    "for i in range((end_date - start_date).days):\n",
    "    df = medium_scraper(tag, current_date)\n",
    "    if i == 0:\n",
    "        df.to_csv('database/medium_data_engineering.csv', index=False)\n",
    "    else:\n",
    "        df.to_csv('database/medium_data_engineering.csv', index=False, mode='a', header=False)\n",
    "    current_date = current_date + dt.timedelta(days=1)\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_med = pd.read_csv('database/medium_data_science.csv')\n",
    "df_med = df_med.append(pd.read_csv('database/medium_machine_learning.csv'))\n",
    "df_med = df_med.append(pd.read_csv('database/medium_data_engineering.csv'))\n",
    "df_med = df_med.drop_duplicates(subset=['id'])\n",
    "df_med['published_date'] = pd.to_datetime(df_med['published_date'], dayfirst=True)\n",
    "df_med = df_med.sort_values(by=['published_date', 'id'])\n",
    "df_med['url'] = df_med['url'].apply(lambda x: x.split('?')[0])\n",
    "df_med = df_med.reset_index(drop=True)\n",
    "df_med.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_med.to_csv('database/medium.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-09 00:00:00\n",
      "2022-03-10 00:00:00\n",
      "2022-03-11 00:00:00\n",
      "2022-03-12 00:00:00\n",
      "2022-03-13 00:00:00\n",
      "2022-03-14 00:00:00\n",
      "2022-03-15 00:00:00\n",
      "2022-03-16 00:00:00\n",
      "2022-03-17 00:00:00\n",
      "2022-03-18 00:00:00\n",
      "2022-03-19 00:00:00\n",
      "2022-03-20 00:00:00\n",
      "2022-03-21 00:00:00\n",
      "2022-03-22 00:00:00\n",
      "2022-03-23 00:00:00\n",
      "2022-03-24 00:00:00\n",
      "2022-03-25 00:00:00\n"
     ]
    }
   ],
   "source": [
    "start_date = dt.datetime(2022, 3, 9)\n",
    "end_date = dt.datetime(2022, 3, 26)\n",
    "current_date = start_date\n",
    "for i in range((end_date - start_date).days):\n",
    "    print(current_date)\n",
    "    current_date = current_date + dt.timedelta(days=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-09 00:00:00\n",
      "2022-03-09 00:00:00\n",
      "2022-03-09 00:00:00\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>subtitle</th>\n",
       "      <th>author</th>\n",
       "      <th>publication</th>\n",
       "      <th>published_date</th>\n",
       "      <th>read_time_mins</th>\n",
       "      <th>claps</th>\n",
       "      <th>url</th>\n",
       "      <th>skills</th>\n",
       "      <th>data_skills</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5b74a24501b0</td>\n",
       "      <td>Best of Both Worlds: Automated and Dynamic SQL...</td>\n",
       "      <td>Bring automation to new¬†heights‚Ä¶</td>\n",
       "      <td>Yi Li</td>\n",
       "      <td>Towards Data Science</td>\n",
       "      <td>2022-03-09</td>\n",
       "      <td>4</td>\n",
       "      <td>160</td>\n",
       "      <td>https://towardsdatascience.com/best-of-both-wo...</td>\n",
       "      <td>Automation; Python; Structured Query Language ...</td>\n",
       "      <td>Python Programming; SQL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8bf616204765</td>\n",
       "      <td>10 SQL Queries You Should Know as a Data Scien...</td>\n",
       "      <td>Learn the Most Used SQL Queries in 5 Minutes w...</td>\n",
       "      <td>Uƒüur Savcƒ±</td>\n",
       "      <td>SelectFrom</td>\n",
       "      <td>2022-03-09</td>\n",
       "      <td>6</td>\n",
       "      <td>159</td>\n",
       "      <td>https://selectfrom.dev/10-sql-queriesyou-shoul...</td>\n",
       "      <td>Structured Query Language (SQL)</td>\n",
       "      <td>SQL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>422a201c6947</td>\n",
       "      <td>7 Important Factors That Affect a Data Scienti...</td>\n",
       "      <td>Factors that determine how much you¬†can‚Ä¶</td>\n",
       "      <td>Chris Zita</td>\n",
       "      <td>Data Science A-Z</td>\n",
       "      <td>2022-03-09</td>\n",
       "      <td>6</td>\n",
       "      <td>119</td>\n",
       "      <td>https://medium.com/data-science-playbook/7-imp...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6c797580b3b1</td>\n",
       "      <td>Network and Interconnection in Python¬†Maps</td>\n",
       "      <td>A hands-on tutorial using Python packages incl...</td>\n",
       "      <td>Himalaya Bir Shrestha</td>\n",
       "      <td>Towards Data Science</td>\n",
       "      <td>2022-03-09</td>\n",
       "      <td>8</td>\n",
       "      <td>131</td>\n",
       "      <td>https://towardsdatascience.com/network-and-int...</td>\n",
       "      <td>Network; Python</td>\n",
       "      <td>Python Programming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ae900e2ac55f</td>\n",
       "      <td>Text Summarization, Part 2‚Ää‚Äî‚ÄäState Of the Art ...</td>\n",
       "      <td>None</td>\n",
       "      <td>Mohamed Bamouh</td>\n",
       "      <td>Besedo Engineering Blog</td>\n",
       "      <td>2022-03-09</td>\n",
       "      <td>9</td>\n",
       "      <td>104</td>\n",
       "      <td>https://medium.com/besedo-engineering/text-sum...</td>\n",
       "      <td>Dataset</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                              title  \\\n",
       "0  5b74a24501b0  Best of Both Worlds: Automated and Dynamic SQL...   \n",
       "1  8bf616204765  10 SQL Queries You Should Know as a Data Scien...   \n",
       "2  422a201c6947  7 Important Factors That Affect a Data Scienti...   \n",
       "3  6c797580b3b1         Network and Interconnection in Python¬†Maps   \n",
       "4  ae900e2ac55f  Text Summarization, Part 2‚Ää‚Äî‚ÄäState Of the Art ...   \n",
       "\n",
       "                                            subtitle                 author  \\\n",
       "0                   Bring automation to new¬†heights‚Ä¶                  Yi Li   \n",
       "1  Learn the Most Used SQL Queries in 5 Minutes w...             Uƒüur Savcƒ±   \n",
       "2           Factors that determine how much you¬†can‚Ä¶             Chris Zita   \n",
       "3  A hands-on tutorial using Python packages incl...  Himalaya Bir Shrestha   \n",
       "4                                               None         Mohamed Bamouh   \n",
       "\n",
       "               publication published_date read_time_mins  claps  \\\n",
       "0     Towards Data Science     2022-03-09              4    160   \n",
       "1               SelectFrom     2022-03-09              6    159   \n",
       "2         Data Science A-Z     2022-03-09              6    119   \n",
       "3     Towards Data Science     2022-03-09              8    131   \n",
       "4  Besedo Engineering Blog     2022-03-09              9    104   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://towardsdatascience.com/best-of-both-wo...   \n",
       "1  https://selectfrom.dev/10-sql-queriesyou-shoul...   \n",
       "2  https://medium.com/data-science-playbook/7-imp...   \n",
       "3  https://towardsdatascience.com/network-and-int...   \n",
       "4  https://medium.com/besedo-engineering/text-sum...   \n",
       "\n",
       "                                              skills              data_skills  \n",
       "0  Automation; Python; Structured Query Language ...  Python Programming; SQL  \n",
       "1                    Structured Query Language (SQL)                      SQL  \n",
       "2                                               None                     None  \n",
       "3                                    Network; Python       Python Programming  \n",
       "4                                            Dataset                     None  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags = ['data-science', 'machine-learning', 'data-engineering']\n",
    "# start_date = dt.datetime(2021, 12, 14)\n",
    "# end_date = dt.datetime(2021, 12, 29)\n",
    "df_med = pd.DataFrame()\n",
    "\n",
    "for tag in tags:\n",
    "    current_date = start_date\n",
    "    print(tag)\n",
    "    for i in range((end_date - start_date).days):\n",
    "        df_temp = medium_scraper(tag, current_date)\n",
    "        df_med = df_med.append(df_temp)\n",
    "        current_date = current_date + dt.timedelta(days=1)\n",
    "        time.sleep(random.randint(1,3))\n",
    "\n",
    "df_med.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>subtitle</th>\n",
       "      <th>author</th>\n",
       "      <th>publication</th>\n",
       "      <th>published_date</th>\n",
       "      <th>read_time_mins</th>\n",
       "      <th>claps</th>\n",
       "      <th>url</th>\n",
       "      <th>skills</th>\n",
       "      <th>data_skills</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2481103aa190</td>\n",
       "      <td>Community-Driven Learning</td>\n",
       "      <td>Learning together is the best way to learn¬†üöÄ</td>\n",
       "      <td>Mert Bozkƒ±r</td>\n",
       "      <td>None</td>\n",
       "      <td>2022-03-09</td>\n",
       "      <td>3</td>\n",
       "      <td>160</td>\n",
       "      <td>https://medium.com/@mertbozkir/community-drive...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2bf2104ccdcc</td>\n",
       "      <td>How to apply an ML model to a large csv in¬†Python</td>\n",
       "      <td>As the size of databases and computational‚Ä¶</td>\n",
       "      <td>Giovanni Valdata</td>\n",
       "      <td>None</td>\n",
       "      <td>2022-03-09</td>\n",
       "      <td>6</td>\n",
       "      <td>263</td>\n",
       "      <td>https://medium.com/@giovanni-valdata/how-to-ap...</td>\n",
       "      <td>Database; Python</td>\n",
       "      <td>Python Programming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>422a201c6947</td>\n",
       "      <td>7 Important Factors That Affect a Data Scienti...</td>\n",
       "      <td>Factors that determine how much you¬†can‚Ä¶</td>\n",
       "      <td>Chris Zita</td>\n",
       "      <td>Data Science A-Z</td>\n",
       "      <td>2022-03-09</td>\n",
       "      <td>6</td>\n",
       "      <td>119</td>\n",
       "      <td>https://medium.com/data-science-playbook/7-imp...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>45af9ed47de9</td>\n",
       "      <td>Predicting Tesla Stocks (TSLA) using Python &amp;¬†...</td>\n",
       "      <td>None</td>\n",
       "      <td>Muneeb Ahmad</td>\n",
       "      <td>Dev Genius</td>\n",
       "      <td>2022-03-09</td>\n",
       "      <td>7</td>\n",
       "      <td>103</td>\n",
       "      <td>https://blog.devgenius.io/predicting-tesla-sto...</td>\n",
       "      <td>Python</td>\n",
       "      <td>Python Programming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>574b655f0bc6</td>\n",
       "      <td>7 Antifragile Principles for a Successful Data...</td>\n",
       "      <td>None</td>\n",
       "      <td>Iliana Iankoulova</td>\n",
       "      <td>Picnic Engineering</td>\n",
       "      <td>2022-03-09</td>\n",
       "      <td>12</td>\n",
       "      <td>176</td>\n",
       "      <td>https://blog.picnic.nl/7-antifragile-principle...</td>\n",
       "      <td>Data Warehouse</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a4730ba8d0e4</td>\n",
       "      <td>Pandas Is Not Enough? A Comprehensive Guide To...</td>\n",
       "      <td>None</td>\n",
       "      <td>Eryk Lewinson</td>\n",
       "      <td>Towards Data Science</td>\n",
       "      <td>2022-03-25</td>\n",
       "      <td>21</td>\n",
       "      <td>407</td>\n",
       "      <td>https://towardsdatascience.com/pandas-is-not-e...</td>\n",
       "      <td>Data Wrangling; Pandas</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a5e993061fc7</td>\n",
       "      <td>Performing Analysis of Meteorological Data</td>\n",
       "      <td>The Hypothesis</td>\n",
       "      <td>Abdullah Abdul Wahid</td>\n",
       "      <td>None</td>\n",
       "      <td>2022-03-25</td>\n",
       "      <td>6</td>\n",
       "      <td>215</td>\n",
       "      <td>https://medium.com/@abdullahw72/performing-ana...</td>\n",
       "      <td>Analysis</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>e615ef62eeb6</td>\n",
       "      <td>Data Security for Dummies in 5¬†mins</td>\n",
       "      <td>Ever heard of ‚ÄúData breach‚Äù? Assuming the answ...</td>\n",
       "      <td>Tanishq Tanwar</td>\n",
       "      <td>None</td>\n",
       "      <td>2022-03-25</td>\n",
       "      <td>4</td>\n",
       "      <td>216</td>\n",
       "      <td>https://medium.com/@akito7011/data-security-fo...</td>\n",
       "      <td>Blogging; Data Security</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>f0350f7c5b6d</td>\n",
       "      <td>Introduction to Text Preprocessing with¬†Python</td>\n",
       "      <td>A beginner‚Äôs guide on text preprocessing with¬†...</td>\n",
       "      <td>Cem Bƒ±kmaz</td>\n",
       "      <td>Python in Plain English</td>\n",
       "      <td>2022-03-25</td>\n",
       "      <td>7</td>\n",
       "      <td>342</td>\n",
       "      <td>https://python.plainenglish.io/introduction-to...</td>\n",
       "      <td>Python</td>\n",
       "      <td>Python Programming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>fe2c73f03107</td>\n",
       "      <td>What languages and cloud platforms are trendin...</td>\n",
       "      <td>Based on over 7,000¬†job‚Ä¶</td>\n",
       "      <td>Leo Walker</td>\n",
       "      <td>InterviewNoodle</td>\n",
       "      <td>2022-03-25</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>https://medium.com/interviewnoodle/what-langua...</td>\n",
       "      <td>Cloud Platform</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>238 rows √ó 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                                              title  \\\n",
       "8   2481103aa190                          Community-Driven Learning   \n",
       "5   2bf2104ccdcc  How to apply an ML model to a large csv in¬†Python   \n",
       "2   422a201c6947  7 Important Factors That Affect a Data Scienti...   \n",
       "0   45af9ed47de9  Predicting Tesla Stocks (TSLA) using Python &¬†...   \n",
       "0   574b655f0bc6  7 Antifragile Principles for a Successful Data...   \n",
       "..           ...                                                ...   \n",
       "0   a4730ba8d0e4  Pandas Is Not Enough? A Comprehensive Guide To...   \n",
       "4   a5e993061fc7         Performing Analysis of Meteorological Data   \n",
       "7   e615ef62eeb6                Data Security for Dummies in 5¬†mins   \n",
       "3   f0350f7c5b6d     Introduction to Text Preprocessing with¬†Python   \n",
       "6   fe2c73f03107  What languages and cloud platforms are trendin...   \n",
       "\n",
       "                                             subtitle                author  \\\n",
       "8        Learning together is the best way to learn¬†üöÄ           Mert Bozkƒ±r   \n",
       "5         As the size of databases and computational‚Ä¶      Giovanni Valdata   \n",
       "2            Factors that determine how much you¬†can‚Ä¶            Chris Zita   \n",
       "0                                                None          Muneeb Ahmad   \n",
       "0                                                None     Iliana Iankoulova   \n",
       "..                                                ...                   ...   \n",
       "0                                                None         Eryk Lewinson   \n",
       "4                                      The Hypothesis  Abdullah Abdul Wahid   \n",
       "7   Ever heard of ‚ÄúData breach‚Äù? Assuming the answ...        Tanishq Tanwar   \n",
       "3   A beginner‚Äôs guide on text preprocessing with¬†...            Cem Bƒ±kmaz   \n",
       "6                            Based on over 7,000¬†job‚Ä¶            Leo Walker   \n",
       "\n",
       "                publication published_date read_time_mins  claps  \\\n",
       "8                      None     2022-03-09              3    160   \n",
       "5                      None     2022-03-09              6    263   \n",
       "2          Data Science A-Z     2022-03-09              6    119   \n",
       "0                Dev Genius     2022-03-09              7    103   \n",
       "0        Picnic Engineering     2022-03-09             12    176   \n",
       "..                      ...            ...            ...    ...   \n",
       "0      Towards Data Science     2022-03-25             21    407   \n",
       "4                      None     2022-03-25              6    215   \n",
       "7                      None     2022-03-25              4    216   \n",
       "3   Python in Plain English     2022-03-25              7    342   \n",
       "6           InterviewNoodle     2022-03-25              3    100   \n",
       "\n",
       "                                                  url  \\\n",
       "8   https://medium.com/@mertbozkir/community-drive...   \n",
       "5   https://medium.com/@giovanni-valdata/how-to-ap...   \n",
       "2   https://medium.com/data-science-playbook/7-imp...   \n",
       "0   https://blog.devgenius.io/predicting-tesla-sto...   \n",
       "0   https://blog.picnic.nl/7-antifragile-principle...   \n",
       "..                                                ...   \n",
       "0   https://towardsdatascience.com/pandas-is-not-e...   \n",
       "4   https://medium.com/@abdullahw72/performing-ana...   \n",
       "7   https://medium.com/@akito7011/data-security-fo...   \n",
       "3   https://python.plainenglish.io/introduction-to...   \n",
       "6   https://medium.com/interviewnoodle/what-langua...   \n",
       "\n",
       "                     skills         data_skills  \n",
       "8                      None                None  \n",
       "5          Database; Python  Python Programming  \n",
       "2                      None                None  \n",
       "0                    Python  Python Programming  \n",
       "0            Data Warehouse                None  \n",
       "..                      ...                 ...  \n",
       "0    Data Wrangling; Pandas                None  \n",
       "4                  Analysis                None  \n",
       "7   Blogging; Data Security                None  \n",
       "3                    Python  Python Programming  \n",
       "6            Cloud Platform                None  \n",
       "\n",
       "[238 rows x 11 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_med = df_med.sort_values(by=['published_date', 'id'])\n",
    "df_med = df_med.drop_duplicates(subset=['id'])\n",
    "df_med"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(settings['skills_db2'])\n",
    "# df_med = pd.read_csv('database/medium.csv')\n",
    "df_med['published_date'] = pd.to_datetime(df_med['published_date'])\n",
    "# df_med.to_sql('ContentMedium', engine, index=False, if_exists='replace')\n",
    "df_med.to_sql('ContentMedium', engine, index=False, if_exists='append')\n",
    "engine.dispose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KDnuggets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kd = pd.read_csv('database/kdnuggets.csv')\n",
    "df_kd['date'] = pd.to_datetime(df_kd['date'])\n",
    "df_kd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kd.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(settings['skills_db2'])\n",
    "df_kd.to_sql('ContentKDnuggets', engine, index=False, if_exists='replace')\n",
    "engine.dispose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.datetime.now() - dt.timedelta(days=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.datetime.now() - dt.timedelta(days=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

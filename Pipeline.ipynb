{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "import datetime as dt\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from sqlalchemy import create_engine\n",
    "from headers import headers_list\n",
    "from data_skills import DATA_SKILLS\n",
    "from skill_extraction import extract_skills, extract_ignore, extract_data_skills\n",
    "from secrets import settings, api_keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JMLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jmlr_scraper(engine):\n",
    "    base_url = 'https://jmlr.org'\n",
    "    url = base_url + '/papers/v22/'\n",
    "    page = requests.get(url, headers=random.choice(headers_list))\n",
    "    if page.status_code != 200:\n",
    "        return\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    dls = soup.findAll('dl')\n",
    "    # Get existing papers in database\n",
    "    df_ex = pd.read_sql_query('select cj.id, cj.title from \"ContentJMLR\" cj', engine)\n",
    "    ex_papers = df_ex['title'].unique().tolist()\n",
    "    papers = []\n",
    "    # Iterate through each paper\n",
    "    has_new = False\n",
    "    for dl in dls:\n",
    "        title = dl.find('dt').get_text()\n",
    "        if title in ex_papers:\n",
    "            continue\n",
    "        paper = {}\n",
    "        dd = dl.find('dd')\n",
    "        paper['title'] = title\n",
    "        paper['authors'] = dd.get_text().split(';')[0].strip()\n",
    "        paper['journal_num'] = dd.get_text().split(';')[-1].split('\\n')[0].strip()\n",
    "        for a in dd.findAll('a'):\n",
    "            if a.get_text() == '(Machine Learning Open Source Software Paper)':\n",
    "                continue\n",
    "            href = a['href']\n",
    "            if 'http' not in href:\n",
    "                href = 'https://jmlr.org' + href\n",
    "            paper[a.get_text()] = href\n",
    "        # Get abstract of paper and extract skills\n",
    "        output = get_abstract_skills(paper)\n",
    "        if output is not None:\n",
    "            paper['abstract'] = output[0]\n",
    "            if len(output[1]) > 0:\n",
    "                paper['skills'] = '; '.join(output[1])\n",
    "                data_skills = extract_data_skills(output[1])\n",
    "                if len(data_skills) > 0:\n",
    "                    paper['data_skills'] = '; '.join(data_skills)\n",
    "        papers.append(paper)\n",
    "        has_new = True\n",
    "    # Compile into dataframe if we have new papers\n",
    "    if has_new:\n",
    "        df = pd.DataFrame.from_dict(papers)\n",
    "        df['id'] = df.index + max(df_ex['id']) + 1\n",
    "        return df\n",
    "    return None\n",
    "\n",
    "def get_abstract_skills(paper):\n",
    "    page = requests.get(paper['abs'], headers=random.choice(headers_list))\n",
    "    if page.status_code != 200:\n",
    "        return None\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    abstract = soup.find('p', class_='abstract').get_text().strip('\\n')\n",
    "    all_skills = extract_skills(paper['title'] + ' ' + abstract)\n",
    "    keep_skills, _ = extract_ignore(all_skills)\n",
    "    keep_skills.sort()\n",
    "    return abstract, keep_skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(settings['skills_db'])\n",
    "df_jmlr = jmlr_scraper(engine)\n",
    "df_jmlr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.dispose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jmlr.to_csv('database/jmlr.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Youtube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_youtube_videos(skill, filter_time=None):\n",
    "    base_url = 'https://www.youtube.com'\n",
    "    # Dictionary for filtering search query\n",
    "    sp_dict = {'this_year': 'EgQIBRAB', 'this_month': 'EgQIBBAB', 'this_week': 'EgQIAxAB', 'today': 'EgQIAhAB'}\n",
    "    if filter_time not in sp_dict.keys():\n",
    "        return None\n",
    "    url = base_url + '/results'\n",
    "    query = 'learn ' + skill\n",
    "    params = {'search_query': query.replace(' ', '+')}\n",
    "    # Default is no filter\n",
    "    if filter_time is not None:\n",
    "        params['sp'] = sp_dict[filter_time]\n",
    "    page = requests.get(url, params=params, headers=random.choice(headers_list))\n",
    "    if page.status_code != 200:\n",
    "        print(page, page.reason)\n",
    "        return None\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    json_text = str(soup.find_all('script')).split('var ytInitialData = ')[-1].split(';</script>')[0]\n",
    "    res = json.loads(json_text)\n",
    "    res = res['contents']['twoColumnSearchResultsRenderer']['primaryContents']['sectionListRenderer']['contents']\n",
    "    video_list = []\n",
    "    # Iterate through each video\n",
    "    for contents in res:\n",
    "        # Get only those with video\n",
    "        if 'itemSectionRenderer' not in contents:\n",
    "            continue\n",
    "        contents = contents['itemSectionRenderer']['contents']\n",
    "        for content in contents:\n",
    "            # Ignore ads\n",
    "            if 'videoRenderer' not in content:\n",
    "                continue\n",
    "            content = content['videoRenderer']\n",
    "            title = get_text(content, 'title')\n",
    "            if title is None:\n",
    "                continue\n",
    "            description = get_description(content)\n",
    "            skills, data_skills = get_skills(title, description)\n",
    "            published_year, published_month = get_published_date(content)\n",
    "            video_list.append({\n",
    "                'id': content['videoId'],\n",
    "                'title': title,\n",
    "                'channel': get_text(content, 'ownerText'),\n",
    "                'published_year': published_year,\n",
    "                'published_month': published_month,\n",
    "                'length': get_length(content),\n",
    "                'view_count': get_view_count(content),\n",
    "                'url': get_url(content),\n",
    "                'description': description,\n",
    "                'skills': skills,\n",
    "                'data_skills': data_skills\n",
    "            })\n",
    "    df = pd.DataFrame.from_dict(video_list)\n",
    "    # df['length'] = pd.to_timedelta(df['length'])\n",
    "    return df\n",
    "\n",
    "def get_text(content, info):\n",
    "    try:\n",
    "        return ' '.join(t['text'] for t in content[info]['runs'])\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def get_length(content):\n",
    "    try:\n",
    "        length = content['lengthText']['simpleText']\n",
    "        length = length.split(':')\n",
    "        if len(length) == 1:\n",
    "            length.insert(0, '00')\n",
    "        if len(length) == 2:\n",
    "            length.insert(0, '00')\n",
    "        return ':'.join(length)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def get_published_date(content):\n",
    "    try:\n",
    "        published_time = content['publishedTimeText']['simpleText']\n",
    "        val = [int(s) for s in published_time.split() if s.isdigit()][0]\n",
    "        current = dt.datetime.now()\n",
    "        if 'year' in published_time:\n",
    "            published = current - dt.timedelta(days=365.25*val)\n",
    "        elif 'month' in published_time:\n",
    "            published = current - dt.timedelta(days=30.436875*val)\n",
    "        elif 'week' in published_time:\n",
    "            published = current - dt.timedelta(weeks=val)\n",
    "        elif 'day' in published_time:\n",
    "            published = current - dt.timedelta(days=val)\n",
    "        elif 'hour' in published_time:\n",
    "            published = current - dt.timedelta(hours=val)\n",
    "        elif 'minute' in published_time:\n",
    "            published = current - dt.timedelta(minutes=val)\n",
    "        elif 'second' in published_time:\n",
    "            published = current - dt.timedelta(seconds=val)\n",
    "        return published.year, published.month\n",
    "    except:\n",
    "        return None, None\n",
    "\n",
    "def get_view_count(content):\n",
    "    try:\n",
    "        view_count = content['viewCountText']['simpleText']\n",
    "        view_count = view_count.split(' views')[0].replace(',', '')\n",
    "        return int(view_count)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def get_url(content):\n",
    "    try:\n",
    "        url = content['navigationEndpoint']['commandMetadata']['webCommandMetadata']['url']\n",
    "        return base_url + url\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def get_description(content):\n",
    "    try:\n",
    "        description = ' '.join([t['text'] for t in content['detailedMetadataSnippets'][0]['snippetText']['runs']])\n",
    "        return description\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def get_skills(title, description):\n",
    "    context = title\n",
    "    if description is not None:\n",
    "        context = context + ' ' + description\n",
    "    all_skills = extract_skills(context)\n",
    "    # Ignore the Video skill as it is not relevant for Youtube\n",
    "    if 'Video' in all_skills:\n",
    "        all_skills.remove('Video')\n",
    "    keep_skills, _ = extract_ignore(all_skills)\n",
    "    keep_skills.sort()\n",
    "    if len(keep_skills) > 0:\n",
    "        data_skills = extract_data_skills(keep_skills)\n",
    "        if len(data_skills) > 0:\n",
    "            return '; '.join(keep_skills), '; '.join(data_skills)\n",
    "        return '; '.join(keep_skills), None\n",
    "    return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yt = pd.DataFrame()\n",
    "\n",
    "for skill in DATA_SKILLS:\n",
    "    try:\n",
    "        df_temp = get_youtube_videos(skill, 'this_month')\n",
    "        df_yt = df_yt.append(df_temp)\n",
    "    except Exception as e:\n",
    "        print('Error in scraping Youtube for {}'.format(skill), e)\n",
    "    time.sleep(5)\n",
    "\n",
    "df_yt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(settings['skills_db2'])\n",
    "df_yt = pd.read_csv('database/youtube.csv')\n",
    "df_yt['length'] = df_yt['length'].apply(lambda x: x.split()[-1])\n",
    "df_yt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yt.to_sql('ContentYoutube', engine, index=False, if_exists='replace')\n",
    "engine.dispose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_yt), len(df_yt[~df_yt['data_skills'].isna()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Medium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def medium_scraper(tag, date):\n",
    "    base_url = 'https://medium.com/tag/{}/archive/'\n",
    "    url = base_url.format(tag) + date.strftime('%Y/%m/%d')\n",
    "    page = requests.get(url, headers=random.choice(headers_list))\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    # Pulls each card from the feed. Each card is a story or comment\n",
    "    cards = soup.find_all('div', class_='streamItem streamItem--postPreview js-streamItem')\n",
    "    card_list = []\n",
    "    for card in cards:\n",
    "        title = get_title(card)\n",
    "        subtitle = get_subtitle(card)\n",
    "        claps = get_claps(card)\n",
    "        if title is None or is_comment(card) or claps is None:\n",
    "            continue\n",
    "        if claps < 100:\n",
    "            continue\n",
    "        skills, data_skills = get_skills(title, subtitle)\n",
    "        card_list.append({\n",
    "            'id': get_id(card),\n",
    "            'title': title,\n",
    "            'subtitle': subtitle,\n",
    "            'author': get_author(card),\n",
    "            'publication': get_publication(card),\n",
    "            'published_date': date,\n",
    "            'read_time_mins': get_read_time(card),\n",
    "            'claps': claps,\n",
    "            'url': get_url(card),\n",
    "            'skills': skills,\n",
    "            'data_skills': data_skills,\n",
    "        })\n",
    "    df = pd.DataFrame.from_dict(card_list)\n",
    "    return df\n",
    "\n",
    "def get_id(card):\n",
    "    id_ = card.find('div', class_='postArticle postArticle--short js-postArticle js-trackPostPresentation js-trackPostScrolls')\n",
    "    if id_ is not None:\n",
    "        return id_['data-post-id']\n",
    "    return id_\n",
    "\n",
    "def get_title(card):\n",
    "    # Different combination of classes possible for titles\n",
    "    combinations = [('h3', 'graf graf--h3 graf-after--figure graf--title'),\n",
    "                    ('h3', 'graf graf--h3 graf-after--figure graf--trailing graf--title'),\n",
    "                    ('h4', 'graf graf--h4 graf--leading'),\n",
    "                    ('h3', 'graf graf--h3 graf--leading graf--title'),\n",
    "                    ('p', 'graf graf--p graf--leading'),\n",
    "                    ('h3', 'graf graf--h3 graf--startsWithDoubleQuote graf--leading graf--title'),\n",
    "                    ('h3', 'graf graf--h3 graf--startsWithDoubleQuote graf-after--figure graf--trailing graf--title')]\n",
    "    title = None\n",
    "    for combi in combinations:\n",
    "        title = card.find(combi[0], class_=combi[1])\n",
    "        if title is not None:\n",
    "            return title.text\n",
    "    return title\n",
    "\n",
    "def get_subtitle(card):\n",
    "    # Different combination of classes possible for subtitles\n",
    "    combinations = [('h4', 'graf graf--h4 graf-after--h3 graf--subtitle'),\n",
    "                    ('h4', 'graf graf--h4 graf-after--h3 graf--trailing graf--subtitle'),\n",
    "                    ('strong', 'markup--strong markup--p-strong'),\n",
    "                    ('h4', 'graf graf--p graf-after--h3 graf--trailing'),\n",
    "                    ('p', 'graf graf--p graf-after--h3 graf--trailing'),\n",
    "                    ('blockquote', 'graf graf--pullquote graf-after--figure graf--trailing'),\n",
    "                    ('p', 'graf graf--p graf-after--figure'),\n",
    "                    ('blockquote', 'graf graf--blockquote graf-after--h3 graf--trailing'),\n",
    "                    ('p', 'graf graf--p graf-after--figure graf--trailing'),\n",
    "                    ('em', 'markup--em markup--p-em'),\n",
    "                    ('p', 'graf graf--p graf-after--p graf--trailing')]\n",
    "    subtitle = None\n",
    "    for combi in combinations:\n",
    "        subtitle = card.find(combi[0], class_=combi[1])\n",
    "        if subtitle is not None:\n",
    "            return subtitle.text\n",
    "    return subtitle\n",
    "\n",
    "def get_author(card):\n",
    "    author = card.find('a', class_='ds-link ds-link--styleSubtle link link--darken link--accent u-accentColor--textNormal u-accentColor--textDarken')\n",
    "    if author is not None:\n",
    "        return author.text\n",
    "    return author\n",
    "\n",
    "def get_publication(card):\n",
    "    pub = card.find('a', class_='ds-link ds-link--styleSubtle link--darken link--accent u-accentColor--textNormal')\n",
    "    if pub is not None:\n",
    "        return pub.text\n",
    "    return pub\n",
    "\n",
    "def get_read_time(card):\n",
    "    time = card.find('span', class_='readingTime')\n",
    "    if time is not None:\n",
    "        time = time['title']\n",
    "        return time.replace(' min read', '')\n",
    "    return time\n",
    "\n",
    "def get_claps(card):\n",
    "    claps = card.find('button', class_='button button--chromeless u-baseColor--buttonNormal js-multirecommendCountButton u-disablePointerEvents')\n",
    "    if claps is not None:\n",
    "        claps = claps.text\n",
    "        if 'K' in claps:\n",
    "            try:\n",
    "                return int(float(claps.replace('K', '')) * 1000)\n",
    "            except:\n",
    "                return None\n",
    "        else:\n",
    "            try:\n",
    "                return int(claps)\n",
    "            except:\n",
    "                return None\n",
    "    return claps\n",
    "\n",
    "def is_comment(card):\n",
    "    # Check if card is a story or comment\n",
    "    comment = card.find('div', class_='u-fontSize14 u-marginTop10 u-marginBottom20 u-padding14 u-xs-padding12 u-borderRadius3 u-borderCardBackground u-borderLighterHover u-boxShadow1px4pxCardBorder')\n",
    "    return comment is not None\n",
    "\n",
    "def get_url(card):\n",
    "    url = card.find('a', class_='')\n",
    "    if url is not None:\n",
    "        return url['href'].split('?')[0]\n",
    "    return url\n",
    "\n",
    "def get_skills(title, subtitle):\n",
    "    context = title\n",
    "    if subtitle is not None:\n",
    "        context = context + ' ' + subtitle\n",
    "    all_skills = extract_skills(context)\n",
    "    keep_skills, _ = extract_ignore(all_skills)\n",
    "    keep_skills.sort()\n",
    "    if len(keep_skills) > 0:\n",
    "        data_skills = extract_data_skills(keep_skills)\n",
    "        if len(data_skills) > 0:\n",
    "            return '; '.join(keep_skills), '; '.join(data_skills)\n",
    "        return '; '.join(keep_skills), None\n",
    "    return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag = 'data-science'\n",
    "start_date = dt.datetime(2021, 9, 30)\n",
    "end_date = dt.datetime(2021, 10, 18)\n",
    "current_date = start_date\n",
    "\n",
    "for i in range((end_date - start_date).days):\n",
    "    df = medium_scraper(tag, current_date)\n",
    "    if i == 0:\n",
    "        df.to_csv('database/medium_data_science.csv', index=False)\n",
    "    else:\n",
    "        df.to_csv('database/medium_data_science.csv', index=False, mode='a', header=False)\n",
    "    current_date = current_date + dt.timedelta(days=1)\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag = 'machine-learning'\n",
    "current_date = start_date\n",
    "\n",
    "for i in range((end_date - start_date).days):\n",
    "    df = medium_scraper(tag, current_date)\n",
    "    if i == 0:\n",
    "        df.to_csv('database/medium_machine_learning.csv', index=False)\n",
    "    else:\n",
    "        df.to_csv('database/medium_machine_learning.csv', index=False, mode='a', header=False)\n",
    "    current_date = current_date + dt.timedelta(days=1)\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag = 'data-engineering'\n",
    "current_date = start_date\n",
    "\n",
    "for i in range((end_date - start_date).days):\n",
    "    df = medium_scraper(tag, current_date)\n",
    "    if i == 0:\n",
    "        df.to_csv('database/medium_data_engineering.csv', index=False)\n",
    "    else:\n",
    "        df.to_csv('database/medium_data_engineering.csv', index=False, mode='a', header=False)\n",
    "    current_date = current_date + dt.timedelta(days=1)\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_med = pd.read_csv('database/medium_data_science.csv')\n",
    "df_med = df_med.append(pd.read_csv('database/medium_machine_learning.csv'))\n",
    "df_med = df_med.append(pd.read_csv('database/medium_data_engineering.csv'))\n",
    "df_med = df_med.drop_duplicates(subset=['id'])\n",
    "df_med['published_date'] = pd.to_datetime(df_med['published_date'], dayfirst=True)\n",
    "df_med = df_med.sort_values(by=['published_date', 'id'])\n",
    "df_med['url'] = df_med['url'].apply(lambda x: x.split('?')[0])\n",
    "df_med = df_med.reset_index(drop=True)\n",
    "df_med.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_med.to_csv('database/medium.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-26 00:00:00\n",
      "2022-03-27 00:00:00\n",
      "2022-03-28 00:00:00\n",
      "2022-03-29 00:00:00\n",
      "2022-03-30 00:00:00\n",
      "2022-03-31 00:00:00\n",
      "2022-04-01 00:00:00\n",
      "2022-04-02 00:00:00\n",
      "2022-04-03 00:00:00\n",
      "2022-04-04 00:00:00\n",
      "2022-04-05 00:00:00\n",
      "2022-04-06 00:00:00\n",
      "2022-04-07 00:00:00\n",
      "2022-04-08 00:00:00\n",
      "2022-04-09 00:00:00\n",
      "2022-04-10 00:00:00\n",
      "2022-04-11 00:00:00\n",
      "2022-04-12 00:00:00\n"
     ]
    }
   ],
   "source": [
    "start_date = dt.datetime(2022, 3, 26)\n",
    "end_date = dt.datetime(2022, 4, 13)\n",
    "current_date = start_date\n",
    "for i in range((end_date - start_date).days):\n",
    "    print(current_date)\n",
    "    current_date = current_date + dt.timedelta(days=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data-science\n",
      "machine-learning\n",
      "data-engineering\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>subtitle</th>\n",
       "      <th>author</th>\n",
       "      <th>publication</th>\n",
       "      <th>published_date</th>\n",
       "      <th>read_time_mins</th>\n",
       "      <th>claps</th>\n",
       "      <th>url</th>\n",
       "      <th>skills</th>\n",
       "      <th>data_skills</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2995910e564</td>\n",
       "      <td>Validate Your pandas DataFrame with¬†Pandera</td>\n",
       "      <td>Make Sure Your Data Matches Your Expectation</td>\n",
       "      <td>Khuyen Tran</td>\n",
       "      <td>Towards Data Science</td>\n",
       "      <td>2022-03-26</td>\n",
       "      <td>5</td>\n",
       "      <td>454</td>\n",
       "      <td>https://towardsdatascience.com/validate-your-p...</td>\n",
       "      <td>Dataframe; Pandas</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fc8867d9b774</td>\n",
       "      <td>How To Really Understand The Raven¬†Paradox?</td>\n",
       "      <td>A paradox that rocks the foundations of scient...</td>\n",
       "      <td>Hemanth</td>\n",
       "      <td>Street Science</td>\n",
       "      <td>2022-03-26</td>\n",
       "      <td>7</td>\n",
       "      <td>293</td>\n",
       "      <td>https://medium.com/street-science/how-to-reall...</td>\n",
       "      <td>Rally</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6b631334ce19</td>\n",
       "      <td>A Rant On Why I Despise Jupyter Notebooks</td>\n",
       "      <td>I remember one day messaging one of my seniors...</td>\n",
       "      <td>Agrover112</td>\n",
       "      <td>CodeX</td>\n",
       "      <td>2022-03-26</td>\n",
       "      <td>4</td>\n",
       "      <td>146</td>\n",
       "      <td>https://medium.com/codex/an-honest-rant-on-why...</td>\n",
       "      <td>Jupyter Notebook</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>38124b21309a</td>\n",
       "      <td>Ba≈üarƒ±lƒ± olmaktan korkma! Veri Bilimci olma yo...</td>\n",
       "      <td>None</td>\n",
       "      <td>Serdar Tafralƒ±</td>\n",
       "      <td>None</td>\n",
       "      <td>2022-03-26</td>\n",
       "      <td>5</td>\n",
       "      <td>109</td>\n",
       "      <td>https://medium.com/@serdartafrali/ba%C5%9Far%C...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>384f49b60d16</td>\n",
       "      <td>Plotly Callbacks: Create Exciting Interactive ...</td>\n",
       "      <td>Get started with callbacks in Plotly¬†Dash</td>\n",
       "      <td>Anmol Tomar</td>\n",
       "      <td>Towards Data Science</td>\n",
       "      <td>2022-03-26</td>\n",
       "      <td>5</td>\n",
       "      <td>145</td>\n",
       "      <td>https://towardsdatascience.com/are-you-still-c...</td>\n",
       "      <td>Dash; Plotly</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                              title  \\\n",
       "0   2995910e564        Validate Your pandas DataFrame with¬†Pandera   \n",
       "1  fc8867d9b774        How To Really Understand The Raven¬†Paradox?   \n",
       "2  6b631334ce19          A Rant On Why I Despise Jupyter Notebooks   \n",
       "3  38124b21309a  Ba≈üarƒ±lƒ± olmaktan korkma! Veri Bilimci olma yo...   \n",
       "4  384f49b60d16  Plotly Callbacks: Create Exciting Interactive ...   \n",
       "\n",
       "                                            subtitle          author  \\\n",
       "0       Make Sure Your Data Matches Your Expectation     Khuyen Tran   \n",
       "1  A paradox that rocks the foundations of scient...         Hemanth   \n",
       "2  I remember one day messaging one of my seniors...      Agrover112   \n",
       "3                                               None  Serdar Tafralƒ±   \n",
       "4          Get started with callbacks in Plotly¬†Dash     Anmol Tomar   \n",
       "\n",
       "            publication published_date read_time_mins  claps  \\\n",
       "0  Towards Data Science     2022-03-26              5    454   \n",
       "1        Street Science     2022-03-26              7    293   \n",
       "2                 CodeX     2022-03-26              4    146   \n",
       "3                  None     2022-03-26              5    109   \n",
       "4  Towards Data Science     2022-03-26              5    145   \n",
       "\n",
       "                                                 url             skills  \\\n",
       "0  https://towardsdatascience.com/validate-your-p...  Dataframe; Pandas   \n",
       "1  https://medium.com/street-science/how-to-reall...              Rally   \n",
       "2  https://medium.com/codex/an-honest-rant-on-why...   Jupyter Notebook   \n",
       "3  https://medium.com/@serdartafrali/ba%C5%9Far%C...               None   \n",
       "4  https://towardsdatascience.com/are-you-still-c...       Dash; Plotly   \n",
       "\n",
       "  data_skills  \n",
       "0        None  \n",
       "1        None  \n",
       "2        None  \n",
       "3        None  \n",
       "4        None  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags = ['data-science', 'machine-learning', 'data-engineering']\n",
    "# start_date = dt.datetime(2021, 12, 14)\n",
    "# end_date = dt.datetime(2021, 12, 29)\n",
    "df_med = pd.DataFrame()\n",
    "\n",
    "for tag in tags:\n",
    "    current_date = start_date\n",
    "    print(tag)\n",
    "    for i in range((end_date - start_date).days):\n",
    "        df_temp = medium_scraper(tag, current_date)\n",
    "        df_med = df_med.append(df_temp)\n",
    "        current_date = current_date + dt.timedelta(days=1)\n",
    "        time.sleep(random.randint(1,3))\n",
    "\n",
    "df_med.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>subtitle</th>\n",
       "      <th>author</th>\n",
       "      <th>publication</th>\n",
       "      <th>published_date</th>\n",
       "      <th>read_time_mins</th>\n",
       "      <th>claps</th>\n",
       "      <th>url</th>\n",
       "      <th>skills</th>\n",
       "      <th>data_skills</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2995910e564</td>\n",
       "      <td>Validate Your pandas DataFrame with¬†Pandera</td>\n",
       "      <td>Make Sure Your Data Matches Your Expectation</td>\n",
       "      <td>Khuyen Tran</td>\n",
       "      <td>Towards Data Science</td>\n",
       "      <td>2022-03-26</td>\n",
       "      <td>5</td>\n",
       "      <td>454</td>\n",
       "      <td>https://towardsdatascience.com/validate-your-p...</td>\n",
       "      <td>Dataframe; Pandas</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2d1e9039f376</td>\n",
       "      <td>Transformer models: an introduction and catalo...</td>\n",
       "      <td>Update 04/02/2022</td>\n",
       "      <td>Xavier Amatriain</td>\n",
       "      <td>None</td>\n",
       "      <td>2022-03-26</td>\n",
       "      <td>17</td>\n",
       "      <td>282</td>\n",
       "      <td>https://medium.com/@xamat/transformers-models-...</td>\n",
       "      <td>Transformer</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>38124b21309a</td>\n",
       "      <td>Ba≈üarƒ±lƒ± olmaktan korkma! Veri Bilimci olma yo...</td>\n",
       "      <td>None</td>\n",
       "      <td>Serdar Tafralƒ±</td>\n",
       "      <td>None</td>\n",
       "      <td>2022-03-26</td>\n",
       "      <td>5</td>\n",
       "      <td>109</td>\n",
       "      <td>https://medium.com/@serdartafrali/ba%C5%9Far%C...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>384f49b60d16</td>\n",
       "      <td>Plotly Callbacks: Create Exciting Interactive ...</td>\n",
       "      <td>Get started with callbacks in Plotly¬†Dash</td>\n",
       "      <td>Anmol Tomar</td>\n",
       "      <td>Towards Data Science</td>\n",
       "      <td>2022-03-26</td>\n",
       "      <td>5</td>\n",
       "      <td>145</td>\n",
       "      <td>https://towardsdatascience.com/are-you-still-c...</td>\n",
       "      <td>Dash; Plotly</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5bf3513e7743</td>\n",
       "      <td>The Most Effective Data Scientists Are Also Et...</td>\n",
       "      <td>None</td>\n",
       "      <td>Rebeca Ansar</td>\n",
       "      <td>An Amygdala</td>\n",
       "      <td>2022-03-26</td>\n",
       "      <td>2</td>\n",
       "      <td>173</td>\n",
       "      <td>https://medium.com/an-amygdala/the-most-effect...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ab8a784aa2d5</td>\n",
       "      <td>Veri Bilimi ile Tavsiye Sistemleri</td>\n",
       "      <td>Tavsiye sistemleri kullanƒ±cƒ±lara bazƒ± teknikle...</td>\n",
       "      <td>Serdar Tafralƒ±</td>\n",
       "      <td>None</td>\n",
       "      <td>2022-04-12</td>\n",
       "      <td>6</td>\n",
       "      <td>131</td>\n",
       "      <td>https://medium.com/@serdartafrali/veri-bilimi-...</td>\n",
       "      <td>C; R</td>\n",
       "      <td>R programming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b33921b4f17f</td>\n",
       "      <td>MLOps or How to Deploy Data Science at¬†Scale</td>\n",
       "      <td>Extending AI &amp; ML in the¬†industry</td>\n",
       "      <td>Thibaud Lamothe ü§†</td>\n",
       "      <td>Towards Data Science</td>\n",
       "      <td>2022-04-12</td>\n",
       "      <td>11</td>\n",
       "      <td>138</td>\n",
       "      <td>https://towardsdatascience.com/mlops-or-how-to...</td>\n",
       "      <td>Artificial Intelligence (AI); Data Science; MLOps</td>\n",
       "      <td>AI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>d49d2590ff9b</td>\n",
       "      <td>Upserting Pandas Dataframes to Snowflake</td>\n",
       "      <td>Automate your pipelines once and for¬†all.</td>\n",
       "      <td>Chlo√© Lagrue</td>\n",
       "      <td>Better Programming</td>\n",
       "      <td>2022-04-12</td>\n",
       "      <td>4</td>\n",
       "      <td>105</td>\n",
       "      <td>https://betterprogramming.pub/upserting-pandas...</td>\n",
       "      <td>Dataframe; Pandas</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d6dbb155673c</td>\n",
       "      <td>Beginner‚Äôs Guide to Machine Learning with Big¬†...</td>\n",
       "      <td>A tutorial for working with large datasets¬†using‚Ä¶</td>\n",
       "      <td>Nathaniel DiRenzo</td>\n",
       "      <td>Towards Data Science</td>\n",
       "      <td>2022-04-12</td>\n",
       "      <td>14</td>\n",
       "      <td>151</td>\n",
       "      <td>https://towardsdatascience.com/beginners-guide...</td>\n",
       "      <td>Big Data; Dataset; Machine Learning</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>d741b5343e04</td>\n",
       "      <td>SQL Server ƒ∞le RFM Analizi-1</td>\n",
       "      <td>‚ÄúCustomer Relationship Management‚Äù</td>\n",
       "      <td>ƒ∞lker Onur I≈üƒ±k</td>\n",
       "      <td>None</td>\n",
       "      <td>2022-04-12</td>\n",
       "      <td>5</td>\n",
       "      <td>151</td>\n",
       "      <td>https://medium.com/@ilkeronurisik/sql-server-i...</td>\n",
       "      <td>Customer Relationship Management (CRM); Struct...</td>\n",
       "      <td>SQL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>235 rows √ó 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                                              title  \\\n",
       "0    2995910e564        Validate Your pandas DataFrame with¬†Pandera   \n",
       "0   2d1e9039f376  Transformer models: an introduction and catalo...   \n",
       "3   38124b21309a  Ba≈üarƒ±lƒ± olmaktan korkma! Veri Bilimci olma yo...   \n",
       "4   384f49b60d16  Plotly Callbacks: Create Exciting Interactive ...   \n",
       "6   5bf3513e7743  The Most Effective Data Scientists Are Also Et...   \n",
       "..           ...                                                ...   \n",
       "11  ab8a784aa2d5                 Veri Bilimi ile Tavsiye Sistemleri   \n",
       "4   b33921b4f17f       MLOps or How to Deploy Data Science at¬†Scale   \n",
       "10  d49d2590ff9b           Upserting Pandas Dataframes to Snowflake   \n",
       "1   d6dbb155673c  Beginner‚Äôs Guide to Machine Learning with Big¬†...   \n",
       "9   d741b5343e04                       SQL Server ƒ∞le RFM Analizi-1   \n",
       "\n",
       "                                             subtitle             author  \\\n",
       "0        Make Sure Your Data Matches Your Expectation        Khuyen Tran   \n",
       "0                                   Update 04/02/2022   Xavier Amatriain   \n",
       "3                                                None     Serdar Tafralƒ±   \n",
       "4           Get started with callbacks in Plotly¬†Dash        Anmol Tomar   \n",
       "6                                                None       Rebeca Ansar   \n",
       "..                                                ...                ...   \n",
       "11  Tavsiye sistemleri kullanƒ±cƒ±lara bazƒ± teknikle...     Serdar Tafralƒ±   \n",
       "4                   Extending AI & ML in the¬†industry  Thibaud Lamothe ü§†   \n",
       "10          Automate your pipelines once and for¬†all.       Chlo√© Lagrue   \n",
       "1   A tutorial for working with large datasets¬†using‚Ä¶  Nathaniel DiRenzo   \n",
       "9                  ‚ÄúCustomer Relationship Management‚Äù    ƒ∞lker Onur I≈üƒ±k   \n",
       "\n",
       "             publication published_date read_time_mins  claps  \\\n",
       "0   Towards Data Science     2022-03-26              5    454   \n",
       "0                   None     2022-03-26             17    282   \n",
       "3                   None     2022-03-26              5    109   \n",
       "4   Towards Data Science     2022-03-26              5    145   \n",
       "6            An Amygdala     2022-03-26              2    173   \n",
       "..                   ...            ...            ...    ...   \n",
       "11                  None     2022-04-12              6    131   \n",
       "4   Towards Data Science     2022-04-12             11    138   \n",
       "10    Better Programming     2022-04-12              4    105   \n",
       "1   Towards Data Science     2022-04-12             14    151   \n",
       "9                   None     2022-04-12              5    151   \n",
       "\n",
       "                                                  url  \\\n",
       "0   https://towardsdatascience.com/validate-your-p...   \n",
       "0   https://medium.com/@xamat/transformers-models-...   \n",
       "3   https://medium.com/@serdartafrali/ba%C5%9Far%C...   \n",
       "4   https://towardsdatascience.com/are-you-still-c...   \n",
       "6   https://medium.com/an-amygdala/the-most-effect...   \n",
       "..                                                ...   \n",
       "11  https://medium.com/@serdartafrali/veri-bilimi-...   \n",
       "4   https://towardsdatascience.com/mlops-or-how-to...   \n",
       "10  https://betterprogramming.pub/upserting-pandas...   \n",
       "1   https://towardsdatascience.com/beginners-guide...   \n",
       "9   https://medium.com/@ilkeronurisik/sql-server-i...   \n",
       "\n",
       "                                               skills    data_skills  \n",
       "0                                   Dataframe; Pandas           None  \n",
       "0                                         Transformer           None  \n",
       "3                                                None           None  \n",
       "4                                        Dash; Plotly           None  \n",
       "6                                                None           None  \n",
       "..                                                ...            ...  \n",
       "11                                               C; R  R programming  \n",
       "4   Artificial Intelligence (AI); Data Science; MLOps             AI  \n",
       "10                                  Dataframe; Pandas           None  \n",
       "1                 Big Data; Dataset; Machine Learning           None  \n",
       "9   Customer Relationship Management (CRM); Struct...            SQL  \n",
       "\n",
       "[235 rows x 11 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_med = df_med.sort_values(by=['published_date', 'id'])\n",
    "df_med = df_med.drop_duplicates(subset=['id'])\n",
    "df_med"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(settings['skills_db2'])\n",
    "# df_med = pd.read_csv('database/medium.csv')\n",
    "df_med['published_date'] = pd.to_datetime(df_med['published_date'])\n",
    "# df_med.to_sql('ContentMedium', engine, index=False, if_exists='replace')\n",
    "df_med.to_sql('ContentMedium', engine, index=False, if_exists='append')\n",
    "engine.dispose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KDnuggets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kd = pd.read_csv('database/kdnuggets.csv')\n",
    "df_kd['date'] = pd.to_datetime(df_kd['date'])\n",
    "df_kd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kd.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(settings['skills_db2'])\n",
    "df_kd.to_sql('ContentKDnuggets', engine, index=False, if_exists='replace')\n",
    "engine.dispose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.datetime.now() - dt.timedelta(days=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.datetime.now() - dt.timedelta(days=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "import datetime as dt\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from headers import headers_list\n",
    "from data_skills import DATA_SKILLS\n",
    "from skill_extraction import extract_skills, extract_ignore, extract_data_skills\n",
    "from secrets import api_keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JMLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jmlr_scraper():\n",
    "    base_url = 'https://jmlr.org'\n",
    "    url = base_url + '/papers/v22/'\n",
    "    page = requests.get(url, headers=random.choice(headers_list))\n",
    "    if page.status_code != 200:\n",
    "        return\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    dls = soup.findAll('dl')\n",
    "    papers = []\n",
    "    # Iterate through each paper\n",
    "    for dl in dls:\n",
    "        paper = {}\n",
    "        paper['title'] = dl.find('dt').get_text()\n",
    "        dd = dl.find('dd')\n",
    "        paper['authors'] = dd.get_text().split(';')[0].strip()\n",
    "        paper['journal_num'] = dd.get_text().split(';')[-1].split('\\n')[0].strip()\n",
    "        for a in dd.findAll('a'):\n",
    "            if a.get_text() == '(Machine Learning Open Source Software Paper)':\n",
    "                continue\n",
    "            href = a['href']\n",
    "            if 'http' not in href:\n",
    "                href = 'https://jmlr.org' + href\n",
    "            paper[a.get_text()] = href\n",
    "        # Get abstract of paper and extract skills\n",
    "        output = get_abstract_skills(paper)\n",
    "        if output is not None:\n",
    "            paper['abstract'] = output[0]\n",
    "            if len(output[1]) > 0:\n",
    "                paper['skills'] = '; '.join(output[1])\n",
    "                data_skills = extract_data_skills(output[1])\n",
    "                if len(data_skills) > 0:\n",
    "                    paper['data_skills'] = '; '.join(data_skills)\n",
    "        papers.append(paper)\n",
    "    df = pd.DataFrame.from_dict(papers)\n",
    "    df['id'] = df.index + 1\n",
    "    df = df[['id', 'title', 'authors', 'journal_num', 'abs', 'pdf', 'bib', 'code', 'supplementary', 'website', 'blog',\n",
    "             'abstract', 'skills', 'data_skills']]\n",
    "    return df\n",
    "\n",
    "def get_abstract_skills(paper):\n",
    "    page = requests.get(paper['abs'], headers=random.choice(headers_list))\n",
    "    if page.status_code != 200:\n",
    "        return None\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    abstract = soup.find('p', class_='abstract').get_text().strip('\\n')\n",
    "    all_skills = extract_skills(paper['title'] + ' ' + abstract)\n",
    "    keep_skills, _ = extract_ignore(all_skills)\n",
    "    keep_skills.sort()\n",
    "    return abstract, keep_skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jmlr = jmlr_scraper()\n",
    "df_jmlr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jmlr.to_csv('database/jmlr.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Youtube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Possible orders: ['date', 'rating', 'relevance', 'title', 'videoCount', 'viewCount']\n",
    "def youtube_scraper(max_results=50, order='relevance'):\n",
    "    # Must be in RFC 3339 formatted date-time value (1970-01-01T00:00:00Z)\n",
    "    # start_date = (dt.datetime.now(dt.timezone.utc) - dt.timedelta(days=365.25)).isoformat()\n",
    "    start_date = dt.datetime(2021, 1, 1).replace(tzinfo=dt.timezone.utc).isoformat()\n",
    "    # Do a search query for each data skills\n",
    "    df = pd.DataFrame()\n",
    "    for skill in DATA_SKILLS:\n",
    "        query = 'learn ' + skill\n",
    "        df_temp = get_youtube_videos(query, max_results, order, start_date)\n",
    "        df_temp['data_skills'] = skill\n",
    "        df = df.append(df_temp)\n",
    "    # Merge df on id to remove duplicates\n",
    "    df = df.merge(df.groupby('id').mean(), left_on='id', right_index=True)\n",
    "    df = df.merge(df[['id', 'data_skills']].groupby('id').agg(lambda x: '; '.join(x)), left_on='id', right_index=True)\n",
    "    df = df.drop_duplicates(subset=['id'])\n",
    "    df = df.drop(columns=['relevance_x', 'data_skills_x'])\n",
    "    df = df.rename(columns={'relevance_y': 'relevance', 'data_skills_y': 'data_skills'})\n",
    "    df = df.reset_index(drop=True)\n",
    "    df = get_info_and_skills_for_videos(df)\n",
    "    return df\n",
    "\n",
    "def get_info_and_skills_for_videos(df):\n",
    "    for i, row in df.iterrows():\n",
    "        full_text = row['title']\n",
    "        video_info = get_video_info(row['id'])\n",
    "        if video_info is None:\n",
    "            continue\n",
    "        full_description = get_snippet(video_info, 'description')\n",
    "        df.loc[i, 'description_full'] = full_description\n",
    "        df.loc[i, 'language'] = get_snippet(video_info, 'defaultAudioLanguage')\n",
    "        df.loc[i, 'view_count'] = get_statistics(video_info, 'viewCount')\n",
    "        df.loc[i, 'like_count'] = get_statistics(video_info, 'likeCount')\n",
    "        df.loc[i, 'dislike_count'] = get_statistics(video_info, 'dislikeCount')\n",
    "        df.loc[i, 'comment_count'] = get_statistics(video_info, 'commentCount')\n",
    "        # Get skills from description\n",
    "        if full_description is not None:\n",
    "            full_text += ' ' + full_description\n",
    "        else:\n",
    "            full_text += ' ' + row['description']\n",
    "        all_skills = extract_skills(full_text)\n",
    "        keep_skills, _ = extract_ignore(all_skills)\n",
    "        keep_skills.sort()\n",
    "        if len(keep_skills) > 0:\n",
    "            df.loc[i, 'skills'] = '; '.join(keep_skills)\n",
    "        data_skills = extract_data_skills(row['data_skills'].split('; ') + keep_skills)\n",
    "        if len(data_skills) > 0:\n",
    "            df.loc[i, 'data_skills'] = '; '.join(data_skills)\n",
    "    return df\n",
    "\n",
    "def get_youtube_videos(query, max_results, order, start_date=None):\n",
    "    base_url = 'https://www.googleapis.com/youtube/v3/search'\n",
    "    params = {'q': query, 'part': 'snippet', 'type': 'video', 'maxResults': max_results, 'order': order,\n",
    "              'key': api_keys['youtube']}\n",
    "    if start_date is not None:\n",
    "        params['publishedAfter'] = start_date\n",
    "    page = requests.get(base_url, params=params, headers=random.choice(headers_list))\n",
    "    if page.status_code != 200:\n",
    "        print(page.reason)\n",
    "        return None\n",
    "    res = json.loads(page.content.decode('utf8'))\n",
    "    video_dict = []\n",
    "    video_url = 'https://youtube.com/watch?v='\n",
    "    for i, item in enumerate(res['items']):\n",
    "        video_id = get_object(item['id'], 'videoId')\n",
    "        snippet = item['snippet']\n",
    "        video_dict.append({\n",
    "            'id': video_id,\n",
    "            'title': get_object(snippet, 'title'),\n",
    "            'channel': get_object(snippet, 'channelTitle'),\n",
    "            'url': video_url + video_id,\n",
    "            'published_date': get_object(snippet, 'publishTime'),\n",
    "            'description': get_object(snippet, 'description'),\n",
    "            'relevance': i+1\n",
    "        })\n",
    "    return pd.DataFrame.from_dict(video_dict)\n",
    "\n",
    "def get_video_info(video_id):\n",
    "    base_url = 'https://www.googleapis.com/youtube/v3/videos'\n",
    "    params = {'part': ['snippet', 'statistics'], 'id': video_id, 'key': api_keys['youtube']}\n",
    "    page = requests.get(base_url, params=params, headers=random.choice(headers_list))\n",
    "    if page.status_code != 200:\n",
    "        return None\n",
    "    try:\n",
    "        res = json.loads(page.content.decode('utf8'))\n",
    "        return res\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def get_object(item, key):\n",
    "    try:\n",
    "        return item[key]\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def get_snippet(res, info):\n",
    "    try:\n",
    "        return res['items'][0]['snippet'][info]\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def get_statistics(res, info):\n",
    "    try:\n",
    "        return res['items'][0]['statistics'][info]\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forbidden\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object does not support item assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-310d74fa25ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0myoutube_scraper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-bb9b793c3092>\u001b[0m in \u001b[0;36myoutube_scraper\u001b[0;34m(max_results, order)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'learn '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mskill\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mdf_temp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_youtube_videos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_results\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_date\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mdf_temp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data_skills'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mskill\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_temp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# Merge df on id to remove duplicates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object does not support item assignment"
     ]
    }
   ],
   "source": [
    "df_yt = youtube_scraper()\n",
    "df_yt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Medium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def medium_scraper(tag, date):\n",
    "    base_url = 'https://medium.com/tag/{}/archive/'\n",
    "    url = base_url.format(tag) + date.strftime('%Y/%m/%d')\n",
    "    page = requests.get(url, headers=random.choice(headers_list))\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    # Pulls each card from the feed. Each card is a story or comment\n",
    "    cards = soup.find_all('div', class_='streamItem streamItem--postPreview js-streamItem')\n",
    "    card_list = []\n",
    "    for card in cards:\n",
    "        title = get_title(card)\n",
    "        subtitle = get_subtitle(card)\n",
    "        claps = get_claps(card)\n",
    "        if title is None or is_comment(card) or claps is None:\n",
    "            continue\n",
    "        if claps < 100:\n",
    "            continue\n",
    "        skills, data_skills = get_skills(title, subtitle)\n",
    "        card_list.append({\n",
    "            'id': get_id(card),\n",
    "            'title': title,\n",
    "            'subtitle': subtitle,\n",
    "            'author': get_author(card),\n",
    "            'publication': get_publication(card),\n",
    "            'published_date': date,\n",
    "            'read_time_mins': get_read_time(card),\n",
    "            'claps': claps,\n",
    "            'url': get_url(card),\n",
    "            'skills': skills,\n",
    "            'data_skills': data_skills,\n",
    "        })\n",
    "    df = pd.DataFrame.from_dict(card_list)\n",
    "    return df\n",
    "\n",
    "def get_id(card):\n",
    "    id_ = card.find('div', class_='postArticle postArticle--short js-postArticle js-trackPostPresentation js-trackPostScrolls')\n",
    "    if id_ is not None:\n",
    "        return id_['data-post-id']\n",
    "    return id_\n",
    "\n",
    "def get_title(card):\n",
    "    # Different combination of classes possible for titles\n",
    "    combinations = [('h3', 'graf graf--h3 graf-after--figure graf--title'),\n",
    "                    ('h3', 'graf graf--h3 graf-after--figure graf--trailing graf--title'),\n",
    "                    ('h4', 'graf graf--h4 graf--leading'),\n",
    "                    ('h3', 'graf graf--h3 graf--leading graf--title'),\n",
    "                    ('p', 'graf graf--p graf--leading'),\n",
    "                    ('h3', 'graf graf--h3 graf--startsWithDoubleQuote graf--leading graf--title'),\n",
    "                    ('h3', 'graf graf--h3 graf--startsWithDoubleQuote graf-after--figure graf--trailing graf--title')]\n",
    "    title = None\n",
    "    for combi in combinations:\n",
    "        title = card.find(combi[0], class_=combi[1])\n",
    "        if title is not None:\n",
    "            return title.text\n",
    "    return title\n",
    "\n",
    "def get_subtitle(card):\n",
    "    # Different combination of classes possible for subtitles\n",
    "    combinations = [('h4', 'graf graf--h4 graf-after--h3 graf--subtitle'),\n",
    "                    ('h4', 'graf graf--h4 graf-after--h3 graf--trailing graf--subtitle'),\n",
    "                    ('strong', 'markup--strong markup--p-strong'),\n",
    "                    ('h4', 'graf graf--p graf-after--h3 graf--trailing'),\n",
    "                    ('p', 'graf graf--p graf-after--h3 graf--trailing'),\n",
    "                    ('blockquote', 'graf graf--pullquote graf-after--figure graf--trailing'),\n",
    "                    ('p', 'graf graf--p graf-after--figure'),\n",
    "                    ('blockquote', 'graf graf--blockquote graf-after--h3 graf--trailing'),\n",
    "                    ('p', 'graf graf--p graf-after--figure graf--trailing'),\n",
    "                    ('em', 'markup--em markup--p-em'),\n",
    "                    ('p', 'graf graf--p graf-after--p graf--trailing')]\n",
    "    subtitle = None\n",
    "    for combi in combinations:\n",
    "        subtitle = card.find(combi[0], class_=combi[1])\n",
    "        if subtitle is not None:\n",
    "            return subtitle.text\n",
    "    return subtitle\n",
    "\n",
    "def get_author(card):\n",
    "    author = card.find('a', class_='ds-link ds-link--styleSubtle link link--darken link--accent u-accentColor--textNormal u-accentColor--textDarken')\n",
    "    if author is not None:\n",
    "        return author.text\n",
    "    return author\n",
    "\n",
    "def get_publication(card):\n",
    "    pub = card.find('a', class_='ds-link ds-link--styleSubtle link--darken link--accent u-accentColor--textNormal')\n",
    "    if pub is not None:\n",
    "        return pub.text\n",
    "    return pub\n",
    "\n",
    "def get_read_time(card):\n",
    "    time = card.find('span', class_='readingTime')\n",
    "    if time is not None:\n",
    "        time = time['title']\n",
    "        return time.replace(' min read', '')\n",
    "    return time\n",
    "\n",
    "def get_claps(card):\n",
    "    claps = card.find('button', class_='button button--chromeless u-baseColor--buttonNormal js-multirecommendCountButton u-disablePointerEvents')\n",
    "    if claps is not None:\n",
    "        claps = claps.text\n",
    "        if 'K' in claps:\n",
    "            try:\n",
    "                return int(float(claps.replace('K', '')) * 1000)\n",
    "            except:\n",
    "                return None\n",
    "        else:\n",
    "            try:\n",
    "                return int(claps)\n",
    "            except:\n",
    "                return None\n",
    "    return claps\n",
    "\n",
    "def is_comment(card):\n",
    "    # Check if card is a story or comment\n",
    "    comment = card.find('div', class_='u-fontSize14 u-marginTop10 u-marginBottom20 u-padding14 u-xs-padding12 u-borderRadius3 u-borderCardBackground u-borderLighterHover u-boxShadow1px4pxCardBorder')\n",
    "    return comment is not None\n",
    "\n",
    "def get_url(card):\n",
    "    url = card.find('a', class_='')\n",
    "    if url is not None:\n",
    "        return url['href'].split('?')[0]\n",
    "    return url\n",
    "\n",
    "def get_skills(title, subtitle):\n",
    "    context = title\n",
    "    if subtitle is not None:\n",
    "        context = context + ' ' + subtitle\n",
    "    all_skills = extract_skills(context)\n",
    "    keep_skills, _ = extract_ignore(all_skills)\n",
    "    keep_skills.sort()\n",
    "    if len(keep_skills) > 0:\n",
    "        data_skills = extract_data_skills(keep_skills)\n",
    "        if len(data_skills) > 0:\n",
    "            return '; '.join(keep_skills), '; '.join(data_skills)\n",
    "        return '; '.join(keep_skills), None\n",
    "    return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag = 'data-science'\n",
    "start_date = dt.datetime(2021, 1, 1)\n",
    "end_date = dt.datetime(2021, 9, 22)\n",
    "current_date = start_date\n",
    "\n",
    "for i in range((end_date - start_date).days):\n",
    "    df = medium_scraper(tag, current_date)\n",
    "    if i == 0:\n",
    "        df.to_csv('database/medium_data_science.csv', index=False)\n",
    "    else:\n",
    "        df.to_csv('database/medium_data_science.csv', index=False, mode='a', header=False)\n",
    "    current_date = current_date + dt.timedelta(days=1)\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag = 'machine-learning'\n",
    "start_date = dt.datetime(2021, 1, 1)\n",
    "end_date = dt.datetime(2021, 9, 22)\n",
    "current_date = start_date\n",
    "\n",
    "for i in range((end_date - start_date).days):\n",
    "    df = medium_scraper(tag, current_date)\n",
    "    if i == 0:\n",
    "        df.to_csv('database/medium_machine_learning.csv', index=False)\n",
    "    else:\n",
    "        df.to_csv('database/medium_machine_learning.csv', index=False, mode='a', header=False)\n",
    "    current_date = current_date + dt.timedelta(days=1)\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>subtitle</th>\n",
       "      <th>author</th>\n",
       "      <th>publication</th>\n",
       "      <th>published_date</th>\n",
       "      <th>read_time_mins</th>\n",
       "      <th>claps</th>\n",
       "      <th>url</th>\n",
       "      <th>skills</th>\n",
       "      <th>data_skills</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>11b6ccd8cd54</td>\n",
       "      <td>Python Betting Model for Six Football Leagues</td>\n",
       "      <td>Using statistics, Pandas, BeautifulSoup and AW...</td>\n",
       "      <td>Liam Hartley</td>\n",
       "      <td>Python in Plain English</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>8</td>\n",
       "      <td>100</td>\n",
       "      <td>https://python.plainenglish.io/python-betting-...</td>\n",
       "      <td>Amazon Web Service (AWS); Pandas; Python; Stat...</td>\n",
       "      <td>Python Programming; Statistics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4776</th>\n",
       "      <td>11b6ccd8cd54</td>\n",
       "      <td>Python Betting Model for Six Football Leagues</td>\n",
       "      <td>Using statistics, Pandas, BeautifulSoup and AW...</td>\n",
       "      <td>Liam Hartley</td>\n",
       "      <td>Python in Plain English</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>8</td>\n",
       "      <td>100</td>\n",
       "      <td>https://python.plainenglish.io/python-betting-...</td>\n",
       "      <td>Amazon Web Service (AWS); Pandas; Python; Stat...</td>\n",
       "      <td>Python Programming; Statistics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>184f9f16f632</td>\n",
       "      <td>Implementing VisualTtransformer in PyTorch</td>\n",
       "      <td>Hi guys, happy new year! Today we are going to...</td>\n",
       "      <td>Francesco Zuppichini</td>\n",
       "      <td>Towards Data Science</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>6</td>\n",
       "      <td>214</td>\n",
       "      <td>https://towardsdatascience.com/implementing-vi...</td>\n",
       "      <td>PyTorch</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4772</th>\n",
       "      <td>184f9f16f632</td>\n",
       "      <td>Implementing VisualTtransformer in PyTorch</td>\n",
       "      <td>Hi guys, happy new year! Today we are going to...</td>\n",
       "      <td>Francesco Zuppichini</td>\n",
       "      <td>Towards Data Science</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>6</td>\n",
       "      <td>214</td>\n",
       "      <td>https://towardsdatascience.com/implementing-vi...</td>\n",
       "      <td>PyTorch</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1de54e479279</td>\n",
       "      <td>Shakespeare versus Eminem— who’s the better ly...</td>\n",
       "      <td>He is known for his poetry, his writings on life…</td>\n",
       "      <td>Jeroen van Zeeland</td>\n",
       "      <td>Towards Data Science</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>9</td>\n",
       "      <td>142</td>\n",
       "      <td>https://towardsdatascience.com/shakespeare-ver...</td>\n",
       "      <td>Writing</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                id                                              title  \\\n",
       "6     11b6ccd8cd54      Python Betting Model for Six Football Leagues   \n",
       "4776  11b6ccd8cd54      Python Betting Model for Six Football Leagues   \n",
       "2     184f9f16f632         Implementing VisualTtransformer in PyTorch   \n",
       "4772  184f9f16f632         Implementing VisualTtransformer in PyTorch   \n",
       "3     1de54e479279  Shakespeare versus Eminem— who’s the better ly...   \n",
       "\n",
       "                                               subtitle                author  \\\n",
       "6     Using statistics, Pandas, BeautifulSoup and AW...          Liam Hartley   \n",
       "4776  Using statistics, Pandas, BeautifulSoup and AW...          Liam Hartley   \n",
       "2     Hi guys, happy new year! Today we are going to...  Francesco Zuppichini   \n",
       "4772  Hi guys, happy new year! Today we are going to...  Francesco Zuppichini   \n",
       "3     He is known for his poetry, his writings on life…    Jeroen van Zeeland   \n",
       "\n",
       "                  publication published_date  read_time_mins  claps  \\\n",
       "6     Python in Plain English     2021-01-01               8    100   \n",
       "4776  Python in Plain English     2021-01-01               8    100   \n",
       "2        Towards Data Science     2021-01-01               6    214   \n",
       "4772     Towards Data Science     2021-01-01               6    214   \n",
       "3        Towards Data Science     2021-01-01               9    142   \n",
       "\n",
       "                                                    url  \\\n",
       "6     https://python.plainenglish.io/python-betting-...   \n",
       "4776  https://python.plainenglish.io/python-betting-...   \n",
       "2     https://towardsdatascience.com/implementing-vi...   \n",
       "4772  https://towardsdatascience.com/implementing-vi...   \n",
       "3     https://towardsdatascience.com/shakespeare-ver...   \n",
       "\n",
       "                                                 skills  \\\n",
       "6     Amazon Web Service (AWS); Pandas; Python; Stat...   \n",
       "4776  Amazon Web Service (AWS); Pandas; Python; Stat...   \n",
       "2                                               PyTorch   \n",
       "4772                                            PyTorch   \n",
       "3                                               Writing   \n",
       "\n",
       "                         data_skills  \n",
       "6     Python Programming; Statistics  \n",
       "4776  Python Programming; Statistics  \n",
       "2                                NaN  \n",
       "4772                             NaN  \n",
       "3                                NaN  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_med = pd.read_csv('database/medium_data_science.csv')\n",
    "df_med = df_med.append(pd.read_csv('database/medium_data_science.csv'))\n",
    "df_med = df_med.reset_index(drop=True)\n",
    "df_med = df_med.sort_values(by=['published_date', 'id'])\n",
    "df_med['url'] = df_med['url'].apply(lambda x: x.split('?')[0])\n",
    "df_med.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['https://python.plainenglish.io/python-betting-model-for-six-football-leagues-11b6ccd8cd54',\n",
       "       'https://towardsdatascience.com/implementing-visualttransformer-in-pytorch-184f9f16f632',\n",
       "       'https://towardsdatascience.com/shakespeare-versus-eminem-1de54e479279',\n",
       "       ...,\n",
       "       'https://towardsdatascience.com/the-easiest-way-to-make-beautiful-interactive-visualizations-with-pandas-cdf6d5e91757',\n",
       "       'https://towardsdatascience.com/how-to-be-a-data-scientist-without-a-stem-degree-ce00b5a66fd4',\n",
       "       'https://towardsdatascience.com/anecdotes-from-11-role-models-in-machine-learning-d01bc0d65dcd'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_med['url'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

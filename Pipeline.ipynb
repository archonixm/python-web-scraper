{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "import datetime as dt\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from sqlalchemy import create_engine\n",
    "from headers import headers_list\n",
    "from data_skills import DATA_SKILLS\n",
    "from skill_extraction import extract_skills, extract_ignore, extract_data_skills\n",
    "from secrets import settings, api_keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JMLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jmlr_scraper(engine):\n",
    "    base_url = 'https://jmlr.org'\n",
    "    url = base_url + '/papers/v22/'\n",
    "    page = requests.get(url, headers=random.choice(headers_list))\n",
    "    if page.status_code != 200:\n",
    "        return\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    dls = soup.findAll('dl')\n",
    "    # Get existing papers in database\n",
    "    df_ex = pd.read_sql_query('select cj.id, cj.title from \"ContentJMLR\" cj', engine)\n",
    "    ex_papers = df_ex['title'].unique().tolist()\n",
    "    papers = []\n",
    "    # Iterate through each paper\n",
    "    has_new = False\n",
    "    for dl in dls:\n",
    "        title = dl.find('dt').get_text()\n",
    "        if title in ex_papers:\n",
    "            continue\n",
    "        paper = {}\n",
    "        dd = dl.find('dd')\n",
    "        paper['title'] = title\n",
    "        paper['authors'] = dd.get_text().split(';')[0].strip()\n",
    "        paper['journal_num'] = dd.get_text().split(';')[-1].split('\\n')[0].strip()\n",
    "        for a in dd.findAll('a'):\n",
    "            if a.get_text() == '(Machine Learning Open Source Software Paper)':\n",
    "                continue\n",
    "            href = a['href']\n",
    "            if 'http' not in href:\n",
    "                href = 'https://jmlr.org' + href\n",
    "            paper[a.get_text()] = href\n",
    "        # Get abstract of paper and extract skills\n",
    "        output = get_abstract_skills(paper)\n",
    "        if output is not None:\n",
    "            paper['abstract'] = output[0]\n",
    "            if len(output[1]) > 0:\n",
    "                paper['skills'] = '; '.join(output[1])\n",
    "                data_skills = extract_data_skills(output[1])\n",
    "                if len(data_skills) > 0:\n",
    "                    paper['data_skills'] = '; '.join(data_skills)\n",
    "        papers.append(paper)\n",
    "        has_new = True\n",
    "    # Compile into dataframe if we have new papers\n",
    "    if has_new:\n",
    "        df = pd.DataFrame.from_dict(papers)\n",
    "        df['id'] = df.index + max(df_ex['id']) + 1\n",
    "        return df\n",
    "    return None\n",
    "\n",
    "def get_abstract_skills(paper):\n",
    "    page = requests.get(paper['abs'], headers=random.choice(headers_list))\n",
    "    if page.status_code != 200:\n",
    "        return None\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    abstract = soup.find('p', class_='abstract').get_text().strip('\\n')\n",
    "    all_skills = extract_skills(paper['title'] + ' ' + abstract)\n",
    "    keep_skills, _ = extract_ignore(all_skills)\n",
    "    keep_skills.sort()\n",
    "    return abstract, keep_skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(settings['skills_db'])\n",
    "df_jmlr = jmlr_scraper(engine)\n",
    "df_jmlr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.dispose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jmlr.to_csv('database/jmlr.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Youtube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_youtube_videos(skill, filter_time=None):\n",
    "    base_url = 'https://www.youtube.com'\n",
    "    # Dictionary for filtering search query\n",
    "    sp_dict = {'this_year': 'EgQIBRAB', 'this_month': 'EgQIBBAB', 'this_week': 'EgQIAxAB', 'today': 'EgQIAhAB'}\n",
    "    if filter_time not in sp_dict.keys():\n",
    "        return None\n",
    "    url = base_url + '/results'\n",
    "    query = 'learn ' + skill\n",
    "    params = {'search_query': query.replace(' ', '+')}\n",
    "    # Default is no filter\n",
    "    if filter_time is not None:\n",
    "        params['sp'] = sp_dict[filter_time]\n",
    "    page = requests.get(url, params=params, headers=random.choice(headers_list))\n",
    "    if page.status_code != 200:\n",
    "        print(page, page.reason)\n",
    "        return None\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    json_text = str(soup.find_all('script')).split('var ytInitialData = ')[-1].split(';</script>')[0]\n",
    "    res = json.loads(json_text)\n",
    "    res = res['contents']['twoColumnSearchResultsRenderer']['primaryContents']['sectionListRenderer']['contents']\n",
    "    video_list = []\n",
    "    # Iterate through each video\n",
    "    for contents in res:\n",
    "        # Get only those with video\n",
    "        if 'itemSectionRenderer' not in contents:\n",
    "            continue\n",
    "        contents = contents['itemSectionRenderer']['contents']\n",
    "        for content in contents:\n",
    "            # Ignore ads\n",
    "            if 'videoRenderer' not in content:\n",
    "                continue\n",
    "            content = content['videoRenderer']\n",
    "            title = get_text(content, 'title')\n",
    "            if title is None:\n",
    "                continue\n",
    "            description = get_description(content)\n",
    "            skills, data_skills = get_skills(title, description)\n",
    "            published_year, published_month = get_published_date(content)\n",
    "            video_list.append({\n",
    "                'id': content['videoId'],\n",
    "                'title': title,\n",
    "                'channel': get_text(content, 'ownerText'),\n",
    "                'published_year': published_year,\n",
    "                'published_month': published_month,\n",
    "                'length': get_length(content),\n",
    "                'view_count': get_view_count(content),\n",
    "                'url': get_url(content),\n",
    "                'description': description,\n",
    "                'skills': skills,\n",
    "                'data_skills': data_skills\n",
    "            })\n",
    "    df = pd.DataFrame.from_dict(video_list)\n",
    "    # df['length'] = pd.to_timedelta(df['length'])\n",
    "    return df\n",
    "\n",
    "def get_text(content, info):\n",
    "    try:\n",
    "        return ' '.join(t['text'] for t in content[info]['runs'])\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def get_length(content):\n",
    "    try:\n",
    "        length = content['lengthText']['simpleText']\n",
    "        length = length.split(':')\n",
    "        if len(length) == 1:\n",
    "            length.insert(0, '00')\n",
    "        if len(length) == 2:\n",
    "            length.insert(0, '00')\n",
    "        return ':'.join(length)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def get_published_date(content):\n",
    "    try:\n",
    "        published_time = content['publishedTimeText']['simpleText']\n",
    "        val = [int(s) for s in published_time.split() if s.isdigit()][0]\n",
    "        current = dt.datetime.now()\n",
    "        if 'year' in published_time:\n",
    "            published = current - dt.timedelta(days=365.25*val)\n",
    "        elif 'month' in published_time:\n",
    "            published = current - dt.timedelta(days=30.436875*val)\n",
    "        elif 'week' in published_time:\n",
    "            published = current - dt.timedelta(weeks=val)\n",
    "        elif 'day' in published_time:\n",
    "            published = current - dt.timedelta(days=val)\n",
    "        elif 'hour' in published_time:\n",
    "            published = current - dt.timedelta(hours=val)\n",
    "        elif 'minute' in published_time:\n",
    "            published = current - dt.timedelta(minutes=val)\n",
    "        elif 'second' in published_time:\n",
    "            published = current - dt.timedelta(seconds=val)\n",
    "        return published.year, published.month\n",
    "    except:\n",
    "        return None, None\n",
    "\n",
    "def get_view_count(content):\n",
    "    try:\n",
    "        view_count = content['viewCountText']['simpleText']\n",
    "        view_count = view_count.split(' views')[0].replace(',', '')\n",
    "        return int(view_count)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def get_url(content):\n",
    "    try:\n",
    "        url = content['navigationEndpoint']['commandMetadata']['webCommandMetadata']['url']\n",
    "        return base_url + url\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def get_description(content):\n",
    "    try:\n",
    "        description = ' '.join([t['text'] for t in content['detailedMetadataSnippets'][0]['snippetText']['runs']])\n",
    "        return description\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def get_skills(title, description):\n",
    "    context = title\n",
    "    if description is not None:\n",
    "        context = context + ' ' + description\n",
    "    all_skills = extract_skills(context)\n",
    "    # Ignore the Video skill as it is not relevant for Youtube\n",
    "    if 'Video' in all_skills:\n",
    "        all_skills.remove('Video')\n",
    "    keep_skills, _ = extract_ignore(all_skills)\n",
    "    keep_skills.sort()\n",
    "    if len(keep_skills) > 0:\n",
    "        data_skills = extract_data_skills(keep_skills)\n",
    "        if len(data_skills) > 0:\n",
    "            return '; '.join(keep_skills), '; '.join(data_skills)\n",
    "        return '; '.join(keep_skills), None\n",
    "    return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yt = pd.DataFrame()\n",
    "\n",
    "for skill in DATA_SKILLS:\n",
    "    try:\n",
    "        df_temp = get_youtube_videos(skill, 'this_month')\n",
    "        df_yt = df_yt.append(df_temp)\n",
    "    except Exception as e:\n",
    "        print('Error in scraping Youtube for {}'.format(skill), e)\n",
    "    time.sleep(5)\n",
    "\n",
    "df_yt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(settings['skills_db2'])\n",
    "df_yt = pd.read_csv('database/youtube.csv')\n",
    "df_yt['length'] = df_yt['length'].apply(lambda x: x.split()[-1])\n",
    "df_yt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yt.to_sql('ContentYoutube', engine, index=False, if_exists='replace')\n",
    "engine.dispose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_yt), len(df_yt[~df_yt['data_skills'].isna()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Medium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def medium_scraper(tag, date):\n",
    "    base_url = 'https://medium.com/tag/{}/archive/'\n",
    "    url = base_url.format(tag) + date.strftime('%Y/%m/%d')\n",
    "    page = requests.get(url, headers=random.choice(headers_list))\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    # Pulls each card from the feed. Each card is a story or comment\n",
    "    cards = soup.find_all('div', class_='streamItem streamItem--postPreview js-streamItem')\n",
    "    card_list = []\n",
    "    for card in cards:\n",
    "        title = get_title(card)\n",
    "        subtitle = get_subtitle(card)\n",
    "        claps = get_claps(card)\n",
    "        if title is None or is_comment(card) or claps is None:\n",
    "            continue\n",
    "        if claps < 100:\n",
    "            continue\n",
    "        skills, data_skills = get_skills(title, subtitle)\n",
    "        card_list.append({\n",
    "            'id': get_id(card),\n",
    "            'title': title,\n",
    "            'subtitle': subtitle,\n",
    "            'author': get_author(card),\n",
    "            'publication': get_publication(card),\n",
    "            'published_date': date,\n",
    "            'read_time_mins': get_read_time(card),\n",
    "            'claps': claps,\n",
    "            'url': get_url(card),\n",
    "            'skills': skills,\n",
    "            'data_skills': data_skills,\n",
    "        })\n",
    "    df = pd.DataFrame.from_dict(card_list)\n",
    "    return df\n",
    "\n",
    "def get_id(card):\n",
    "    id_ = card.find('div', class_='postArticle postArticle--short js-postArticle js-trackPostPresentation js-trackPostScrolls')\n",
    "    if id_ is not None:\n",
    "        return id_['data-post-id']\n",
    "    return id_\n",
    "\n",
    "def get_title(card):\n",
    "    # Different combination of classes possible for titles\n",
    "    combinations = [('h3', 'graf graf--h3 graf-after--figure graf--title'),\n",
    "                    ('h3', 'graf graf--h3 graf-after--figure graf--trailing graf--title'),\n",
    "                    ('h4', 'graf graf--h4 graf--leading'),\n",
    "                    ('h3', 'graf graf--h3 graf--leading graf--title'),\n",
    "                    ('p', 'graf graf--p graf--leading'),\n",
    "                    ('h3', 'graf graf--h3 graf--startsWithDoubleQuote graf--leading graf--title'),\n",
    "                    ('h3', 'graf graf--h3 graf--startsWithDoubleQuote graf-after--figure graf--trailing graf--title')]\n",
    "    title = None\n",
    "    for combi in combinations:\n",
    "        title = card.find(combi[0], class_=combi[1])\n",
    "        if title is not None:\n",
    "            return title.text\n",
    "    return title\n",
    "\n",
    "def get_subtitle(card):\n",
    "    # Different combination of classes possible for subtitles\n",
    "    combinations = [('h4', 'graf graf--h4 graf-after--h3 graf--subtitle'),\n",
    "                    ('h4', 'graf graf--h4 graf-after--h3 graf--trailing graf--subtitle'),\n",
    "                    ('strong', 'markup--strong markup--p-strong'),\n",
    "                    ('h4', 'graf graf--p graf-after--h3 graf--trailing'),\n",
    "                    ('p', 'graf graf--p graf-after--h3 graf--trailing'),\n",
    "                    ('blockquote', 'graf graf--pullquote graf-after--figure graf--trailing'),\n",
    "                    ('p', 'graf graf--p graf-after--figure'),\n",
    "                    ('blockquote', 'graf graf--blockquote graf-after--h3 graf--trailing'),\n",
    "                    ('p', 'graf graf--p graf-after--figure graf--trailing'),\n",
    "                    ('em', 'markup--em markup--p-em'),\n",
    "                    ('p', 'graf graf--p graf-after--p graf--trailing')]\n",
    "    subtitle = None\n",
    "    for combi in combinations:\n",
    "        subtitle = card.find(combi[0], class_=combi[1])\n",
    "        if subtitle is not None:\n",
    "            return subtitle.text\n",
    "    return subtitle\n",
    "\n",
    "def get_author(card):\n",
    "    author = card.find('a', class_='ds-link ds-link--styleSubtle link link--darken link--accent u-accentColor--textNormal u-accentColor--textDarken')\n",
    "    if author is not None:\n",
    "        return author.text\n",
    "    return author\n",
    "\n",
    "def get_publication(card):\n",
    "    pub = card.find('a', class_='ds-link ds-link--styleSubtle link--darken link--accent u-accentColor--textNormal')\n",
    "    if pub is not None:\n",
    "        return pub.text\n",
    "    return pub\n",
    "\n",
    "def get_read_time(card):\n",
    "    time = card.find('span', class_='readingTime')\n",
    "    if time is not None:\n",
    "        time = time['title']\n",
    "        return time.replace(' min read', '')\n",
    "    return time\n",
    "\n",
    "def get_claps(card):\n",
    "    claps = card.find('button', class_='button button--chromeless u-baseColor--buttonNormal js-multirecommendCountButton u-disablePointerEvents')\n",
    "    if claps is not None:\n",
    "        claps = claps.text\n",
    "        if 'K' in claps:\n",
    "            try:\n",
    "                return int(float(claps.replace('K', '')) * 1000)\n",
    "            except:\n",
    "                return None\n",
    "        else:\n",
    "            try:\n",
    "                return int(claps)\n",
    "            except:\n",
    "                return None\n",
    "    return claps\n",
    "\n",
    "def is_comment(card):\n",
    "    # Check if card is a story or comment\n",
    "    comment = card.find('div', class_='u-fontSize14 u-marginTop10 u-marginBottom20 u-padding14 u-xs-padding12 u-borderRadius3 u-borderCardBackground u-borderLighterHover u-boxShadow1px4pxCardBorder')\n",
    "    return comment is not None\n",
    "\n",
    "def get_url(card):\n",
    "    url = card.find('a', class_='')\n",
    "    if url is not None:\n",
    "        return url['href'].split('?')[0]\n",
    "    return url\n",
    "\n",
    "def get_skills(title, subtitle):\n",
    "    context = title\n",
    "    if subtitle is not None:\n",
    "        context = context + ' ' + subtitle\n",
    "    all_skills = extract_skills(context)\n",
    "    keep_skills, _ = extract_ignore(all_skills)\n",
    "    keep_skills.sort()\n",
    "    if len(keep_skills) > 0:\n",
    "        data_skills = extract_data_skills(keep_skills)\n",
    "        if len(data_skills) > 0:\n",
    "            return '; '.join(keep_skills), '; '.join(data_skills)\n",
    "        return '; '.join(keep_skills), None\n",
    "    return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-155612f82bed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_date\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_date\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmedium_scraper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_date\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'database/medium_data_science.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-57b032503617>\u001b[0m in \u001b[0;36mmedium_scraper\u001b[0;34m(tag, date)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mclaps\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mskills\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_skills\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_skills\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubtitle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         card_list.append({\n\u001b[1;32m     19\u001b[0m             \u001b[0;34m'id'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mget_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcard\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-57b032503617>\u001b[0m in \u001b[0;36mget_skills\u001b[0;34m(title, subtitle)\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msubtitle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msubtitle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0mall_skills\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_skills\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m     \u001b[0mkeep_skills\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_ignore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_skills\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0mkeep_skills\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/python-web-scraper/skill_extraction.py\u001b[0m in \u001b[0;36mextract_skills\u001b[0;34m(info, threshold)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;31m# Skill consist of two words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgcm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbigrams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcutoff\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m                 \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mskill\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;31m# Skill consist of three words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/difflib.py\u001b[0m in \u001b[0;36mget_close_matches\u001b[0;34m(word, possibilities, n, cutoff)\u001b[0m\n\u001b[1;32m    725\u001b[0m         \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_seq1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreal_quick_ratio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mcutoff\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m            \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquick_ratio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mcutoff\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m            \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mratio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mcutoff\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m             \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mratio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/difflib.py\u001b[0m in \u001b[0;36mquick_ratio\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m                 \u001b[0mnumb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfullbcount\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m             \u001b[0mavail\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0melt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumb\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnumb\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m                 \u001b[0mmatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatches\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tag = 'data-science'\n",
    "start_date = dt.datetime(2021, 9, 30)\n",
    "end_date = dt.datetime(2021, 10, 18)\n",
    "current_date = start_date\n",
    "\n",
    "for i in range((end_date - start_date).days):\n",
    "    df = medium_scraper(tag, current_date)\n",
    "    if i == 0:\n",
    "        df.to_csv('database/medium_data_science.csv', index=False)\n",
    "    else:\n",
    "        df.to_csv('database/medium_data_science.csv', index=False, mode='a', header=False)\n",
    "    current_date = current_date + dt.timedelta(days=1)\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag = 'machine-learning'\n",
    "current_date = start_date\n",
    "\n",
    "for i in range((end_date - start_date).days):\n",
    "    df = medium_scraper(tag, current_date)\n",
    "    if i == 0:\n",
    "        df.to_csv('database/medium_machine_learning.csv', index=False)\n",
    "    else:\n",
    "        df.to_csv('database/medium_machine_learning.csv', index=False, mode='a', header=False)\n",
    "    current_date = current_date + dt.timedelta(days=1)\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag = 'data-engineering'\n",
    "current_date = start_date\n",
    "\n",
    "for i in range((end_date - start_date).days):\n",
    "    df = medium_scraper(tag, current_date)\n",
    "    if i == 0:\n",
    "        df.to_csv('database/medium_data_engineering.csv', index=False)\n",
    "    else:\n",
    "        df.to_csv('database/medium_data_engineering.csv', index=False, mode='a', header=False)\n",
    "    current_date = current_date + dt.timedelta(days=1)\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_med = pd.read_csv('database/medium_data_science.csv')\n",
    "df_med = df_med.append(pd.read_csv('database/medium_machine_learning.csv'))\n",
    "df_med = df_med.append(pd.read_csv('database/medium_data_engineering.csv'))\n",
    "df_med = df_med.drop_duplicates(subset=['id'])\n",
    "df_med['published_date'] = pd.to_datetime(df_med['published_date'], dayfirst=True)\n",
    "df_med = df_med.sort_values(by=['published_date', 'id'])\n",
    "df_med['url'] = df_med['url'].apply(lambda x: x.split('?')[0])\n",
    "df_med = df_med.reset_index(drop=True)\n",
    "df_med.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_med.to_csv('database/medium.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>subtitle</th>\n",
       "      <th>author</th>\n",
       "      <th>publication</th>\n",
       "      <th>published_date</th>\n",
       "      <th>read_time_mins</th>\n",
       "      <th>claps</th>\n",
       "      <th>url</th>\n",
       "      <th>skills</th>\n",
       "      <th>data_skills</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cd1bcd607426</td>\n",
       "      <td>Pandas Cheat Sheet for Data Preprocessing</td>\n",
       "      <td>Practical guide about how to preprocess data w...</td>\n",
       "      <td>Ryota Kiuchi</td>\n",
       "      <td>Towards Data Science</td>\n",
       "      <td>2021-09-30</td>\n",
       "      <td>8</td>\n",
       "      <td>289</td>\n",
       "      <td>https://towardsdatascience.com/pandas-cheat-sh...</td>\n",
       "      <td>Pandas</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3b6b4e00d608</td>\n",
       "      <td>Snowflake Vs BigQuery — Two Cloud Data Warehou...</td>\n",
       "      <td>Setting Up Your Data In The Modern Data…</td>\n",
       "      <td>SeattleDataGuy</td>\n",
       "      <td>SeattleDataGuy</td>\n",
       "      <td>2021-09-30</td>\n",
       "      <td>6</td>\n",
       "      <td>118</td>\n",
       "      <td>https://medium.com/coriers/snowflake-vs-bigque...</td>\n",
       "      <td>Data Warehouse; Google BigQuery</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>854be0f9fe9d</td>\n",
       "      <td>How Airbnb Tech Fosters a Culture of Learning</td>\n",
       "      <td>Leveraging technical learning and development ...</td>\n",
       "      <td>Tamera Scholz</td>\n",
       "      <td>The Airbnb Tech Blog</td>\n",
       "      <td>2021-09-30</td>\n",
       "      <td>8</td>\n",
       "      <td>237</td>\n",
       "      <td>https://medium.com/airbnb-engineering/how-airb...</td>\n",
       "      <td>Development Tools</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>78305cb2732b</td>\n",
       "      <td>Multiple Indicator Trading Strategy in Python ...</td>\n",
       "      <td>Creating a Trading Strategy Based on…</td>\n",
       "      <td>Sofien Kaabar</td>\n",
       "      <td>Investor’s Handbook</td>\n",
       "      <td>2021-09-30</td>\n",
       "      <td>14</td>\n",
       "      <td>104</td>\n",
       "      <td>https://medium.com/the-investors-handbook/mult...</td>\n",
       "      <td>Python</td>\n",
       "      <td>Python Programming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bf56d46b2abe</td>\n",
       "      <td>Deploy a Public Streamlit Web App for Free — H...</td>\n",
       "      <td>Google Sheets as its backend and hosted by…</td>\n",
       "      <td>Yong Cui</td>\n",
       "      <td>Towards Data Science</td>\n",
       "      <td>2021-09-30</td>\n",
       "      <td>7</td>\n",
       "      <td>156</td>\n",
       "      <td>https://towardsdatascience.com/deploy-a-public...</td>\n",
       "      <td>Google Sheets</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                              title  \\\n",
       "0  cd1bcd607426          Pandas Cheat Sheet for Data Preprocessing   \n",
       "1  3b6b4e00d608  Snowflake Vs BigQuery — Two Cloud Data Warehou...   \n",
       "2  854be0f9fe9d      How Airbnb Tech Fosters a Culture of Learning   \n",
       "3  78305cb2732b  Multiple Indicator Trading Strategy in Python ...   \n",
       "4  bf56d46b2abe  Deploy a Public Streamlit Web App for Free — H...   \n",
       "\n",
       "                                            subtitle          author  \\\n",
       "0  Practical guide about how to preprocess data w...    Ryota Kiuchi   \n",
       "1           Setting Up Your Data In The Modern Data…  SeattleDataGuy   \n",
       "2  Leveraging technical learning and development ...   Tamera Scholz   \n",
       "3              Creating a Trading Strategy Based on…   Sofien Kaabar   \n",
       "4        Google Sheets as its backend and hosted by…        Yong Cui   \n",
       "\n",
       "            publication published_date read_time_mins  claps  \\\n",
       "0  Towards Data Science     2021-09-30              8    289   \n",
       "1        SeattleDataGuy     2021-09-30              6    118   \n",
       "2  The Airbnb Tech Blog     2021-09-30              8    237   \n",
       "3   Investor’s Handbook     2021-09-30             14    104   \n",
       "4  Towards Data Science     2021-09-30              7    156   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://towardsdatascience.com/pandas-cheat-sh...   \n",
       "1  https://medium.com/coriers/snowflake-vs-bigque...   \n",
       "2  https://medium.com/airbnb-engineering/how-airb...   \n",
       "3  https://medium.com/the-investors-handbook/mult...   \n",
       "4  https://towardsdatascience.com/deploy-a-public...   \n",
       "\n",
       "                            skills         data_skills  \n",
       "0                           Pandas                None  \n",
       "1  Data Warehouse; Google BigQuery                None  \n",
       "2                Development Tools                None  \n",
       "3                           Python  Python Programming  \n",
       "4                    Google Sheets                None  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags = ['data-science', 'machine-learning', 'data-engineering']\n",
    "start_date = dt.datetime(2021, 9, 30)\n",
    "end_date = dt.datetime(2021, 10, 18)\n",
    "df_med = pd.DataFrame()\n",
    "\n",
    "for tag in tags:\n",
    "    current_date = start_date\n",
    "    for i in range((end_date - start_date).days):\n",
    "        df_temp = medium_scraper(tag, current_date)\n",
    "        df_med = df_med.append(df_temp)\n",
    "        current_date = current_date + dt.timedelta(days=1)\n",
    "        time.sleep(3)\n",
    "\n",
    "df_med.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>subtitle</th>\n",
       "      <th>author</th>\n",
       "      <th>publication</th>\n",
       "      <th>published_date</th>\n",
       "      <th>read_time_mins</th>\n",
       "      <th>claps</th>\n",
       "      <th>url</th>\n",
       "      <th>skills</th>\n",
       "      <th>data_skills</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1aad64f71e34</td>\n",
       "      <td>The best Matplotlib cheat sheet!</td>\n",
       "      <td>Welcome back! Matplotlib is one of the most im...</td>\n",
       "      <td>Manpreet Singh</td>\n",
       "      <td>None</td>\n",
       "      <td>2021-09-30</td>\n",
       "      <td>3</td>\n",
       "      <td>103</td>\n",
       "      <td>https://medium.com/@preettheman/the-best-matpl...</td>\n",
       "      <td>Matplotlib; Python</td>\n",
       "      <td>Python Programming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1b0fe4a8b39</td>\n",
       "      <td>The data investment life cycle</td>\n",
       "      <td>And the importance of J-curve for investors. A...</td>\n",
       "      <td>Adam Votava</td>\n",
       "      <td>None</td>\n",
       "      <td>2021-09-30</td>\n",
       "      <td>4</td>\n",
       "      <td>105</td>\n",
       "      <td>https://medium.com/@adamvotava/the-data-invest...</td>\n",
       "      <td>Investment</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>20806356385e</td>\n",
       "      <td>How Accurate is Amazon Transcribe on South Afr...</td>\n",
       "      <td>Measuring transcription accuracy when…</td>\n",
       "      <td>Nick Wilkinson</td>\n",
       "      <td>Tesserae AI</td>\n",
       "      <td>2021-09-30</td>\n",
       "      <td>10</td>\n",
       "      <td>115</td>\n",
       "      <td>https://medium.com/tesserae-ai/how-accurate-is...</td>\n",
       "      <td>Accuracy; English</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3b6b4e00d608</td>\n",
       "      <td>Snowflake Vs BigQuery — Two Cloud Data Warehou...</td>\n",
       "      <td>Setting Up Your Data In The Modern Data…</td>\n",
       "      <td>SeattleDataGuy</td>\n",
       "      <td>SeattleDataGuy</td>\n",
       "      <td>2021-09-30</td>\n",
       "      <td>6</td>\n",
       "      <td>118</td>\n",
       "      <td>https://medium.com/coriers/snowflake-vs-bigque...</td>\n",
       "      <td>Data Warehouse; Google BigQuery</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3ee6cf26f59</td>\n",
       "      <td>Como Machine Learning pode ajudar seu negócio ...</td>\n",
       "      <td>None</td>\n",
       "      <td>Hernane Braga</td>\n",
       "      <td>BIX Tecnologia</td>\n",
       "      <td>2021-09-30</td>\n",
       "      <td>14</td>\n",
       "      <td>357</td>\n",
       "      <td>https://medium.com/bix-tecnologia/como-machine...</td>\n",
       "      <td>Machine Learning</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>b7bc048081c9</td>\n",
       "      <td>Big Data - An Introduction</td>\n",
       "      <td>Big Data is a process of storing, analyzing, a...</td>\n",
       "      <td>Swaathi Sundaramurugan</td>\n",
       "      <td>None</td>\n",
       "      <td>2021-10-17</td>\n",
       "      <td>7</td>\n",
       "      <td>159</td>\n",
       "      <td>https://medium.com/@swaathi317/big-data-an-int...</td>\n",
       "      <td>Big Data</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>c38ff75ba02a</td>\n",
       "      <td>9 Browser Tools That Will Make Your Life as a ...</td>\n",
       "      <td>Amazing developer tools that help you…</td>\n",
       "      <td>Josef Cruz</td>\n",
       "      <td>JavaScript in Plain English</td>\n",
       "      <td>2021-10-17</td>\n",
       "      <td>6</td>\n",
       "      <td>147</td>\n",
       "      <td>https://javascript.plainenglish.io/9-browser-t...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>c9b17faabfb0</td>\n",
       "      <td>Parametric Tests — the t-test</td>\n",
       "      <td>An intro to parametric tests and a deep dive i...</td>\n",
       "      <td>Shubhangi Hora</td>\n",
       "      <td>None</td>\n",
       "      <td>2021-10-17</td>\n",
       "      <td>14</td>\n",
       "      <td>106</td>\n",
       "      <td>https://medium.com/@shubhangihora/parametric-t...</td>\n",
       "      <td>Parametric Test</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d7eb3b41b486</td>\n",
       "      <td>How you can use Python to find the top ten mom...</td>\n",
       "      <td>None</td>\n",
       "      <td>Anthony Wynne</td>\n",
       "      <td>None</td>\n",
       "      <td>2021-10-17</td>\n",
       "      <td>8</td>\n",
       "      <td>133</td>\n",
       "      <td>https://medium.com/@anthony_wynne/how-you-can-...</td>\n",
       "      <td>Python</td>\n",
       "      <td>Python Programming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>f775c9432887</td>\n",
       "      <td>Python 3.10 — Five New Features And Considerat...</td>\n",
       "      <td>Not only listing but also the examples and…</td>\n",
       "      <td>Christopher Tao</td>\n",
       "      <td>Towards Data Science</td>\n",
       "      <td>2021-10-17</td>\n",
       "      <td>8</td>\n",
       "      <td>181</td>\n",
       "      <td>https://towardsdatascience.com/python-3-10-fiv...</td>\n",
       "      <td>Python</td>\n",
       "      <td>Python Programming</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                                              title  \\\n",
       "8   1aad64f71e34                   The best Matplotlib cheat sheet!   \n",
       "7    1b0fe4a8b39                     The data investment life cycle   \n",
       "8   20806356385e  How Accurate is Amazon Transcribe on South Afr...   \n",
       "1   3b6b4e00d608  Snowflake Vs BigQuery — Two Cloud Data Warehou...   \n",
       "10   3ee6cf26f59  Como Machine Learning pode ajudar seu negócio ...   \n",
       "..           ...                                                ...   \n",
       "7   b7bc048081c9                         Big Data - An Introduction   \n",
       "3   c38ff75ba02a  9 Browser Tools That Will Make Your Life as a ...   \n",
       "5   c9b17faabfb0                      Parametric Tests — the t-test   \n",
       "1   d7eb3b41b486  How you can use Python to find the top ten mom...   \n",
       "0   f775c9432887  Python 3.10 — Five New Features And Considerat...   \n",
       "\n",
       "                                             subtitle                  author  \\\n",
       "8   Welcome back! Matplotlib is one of the most im...          Manpreet Singh   \n",
       "7   And the importance of J-curve for investors. A...             Adam Votava   \n",
       "8              Measuring transcription accuracy when…          Nick Wilkinson   \n",
       "1            Setting Up Your Data In The Modern Data…          SeattleDataGuy   \n",
       "10                                               None           Hernane Braga   \n",
       "..                                                ...                     ...   \n",
       "7   Big Data is a process of storing, analyzing, a...  Swaathi Sundaramurugan   \n",
       "3              Amazing developer tools that help you…              Josef Cruz   \n",
       "5   An intro to parametric tests and a deep dive i...          Shubhangi Hora   \n",
       "1                                                None           Anthony Wynne   \n",
       "0         Not only listing but also the examples and…         Christopher Tao   \n",
       "\n",
       "                    publication published_date read_time_mins  claps  \\\n",
       "8                          None     2021-09-30              3    103   \n",
       "7                          None     2021-09-30              4    105   \n",
       "8                   Tesserae AI     2021-09-30             10    115   \n",
       "1                SeattleDataGuy     2021-09-30              6    118   \n",
       "10               BIX Tecnologia     2021-09-30             14    357   \n",
       "..                          ...            ...            ...    ...   \n",
       "7                          None     2021-10-17              7    159   \n",
       "3   JavaScript in Plain English     2021-10-17              6    147   \n",
       "5                          None     2021-10-17             14    106   \n",
       "1                          None     2021-10-17              8    133   \n",
       "0          Towards Data Science     2021-10-17              8    181   \n",
       "\n",
       "                                                  url  \\\n",
       "8   https://medium.com/@preettheman/the-best-matpl...   \n",
       "7   https://medium.com/@adamvotava/the-data-invest...   \n",
       "8   https://medium.com/tesserae-ai/how-accurate-is...   \n",
       "1   https://medium.com/coriers/snowflake-vs-bigque...   \n",
       "10  https://medium.com/bix-tecnologia/como-machine...   \n",
       "..                                                ...   \n",
       "7   https://medium.com/@swaathi317/big-data-an-int...   \n",
       "3   https://javascript.plainenglish.io/9-browser-t...   \n",
       "5   https://medium.com/@shubhangihora/parametric-t...   \n",
       "1   https://medium.com/@anthony_wynne/how-you-can-...   \n",
       "0   https://towardsdatascience.com/python-3-10-fiv...   \n",
       "\n",
       "                             skills         data_skills  \n",
       "8                Matplotlib; Python  Python Programming  \n",
       "7                        Investment                None  \n",
       "8                 Accuracy; English                None  \n",
       "1   Data Warehouse; Google BigQuery                None  \n",
       "10                 Machine Learning                None  \n",
       "..                              ...                 ...  \n",
       "7                          Big Data                None  \n",
       "3                              None                None  \n",
       "5                   Parametric Test                None  \n",
       "1                            Python  Python Programming  \n",
       "0                            Python  Python Programming  \n",
       "\n",
       "[300 rows x 11 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_med = df_med.sort_values(by=['published_date', 'id'])\n",
    "df_med = df_med.drop_duplicates(subset=['id'])\n",
    "df_med"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(settings['skills_db2'])\n",
    "# df_med = pd.read_csv('database/medium.csv')\n",
    "df_med['published_date'] = pd.to_datetime(df_med['published_date'])\n",
    "# df_med.to_sql('ContentMedium', engine, index=False, if_exists='replace')\n",
    "df_med.to_sql('ContentMedium', engine, index=False, if_exists='append')\n",
    "engine.dispose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KDnuggets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kd = pd.read_csv('database/kdnuggets.csv')\n",
    "df_kd['date'] = pd.to_datetime(df_kd['date'])\n",
    "df_kd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kd.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(settings['skills_db2'])\n",
    "df_kd.to_sql('ContentKDnuggets', engine, index=False, if_exists='replace')\n",
    "engine.dispose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.datetime.now() - dt.timedelta(days=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.datetime.now() - dt.timedelta(days=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
